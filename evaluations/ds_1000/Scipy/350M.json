[
    {
        "library_id": 0,
        "source_id": 0,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a set of data and I want to compare which line describes it best (polynomials of different orders, exponential or logarithmic).\nI use Python and Numpy and for polynomial fitting there is a function polyfit(). \nHow do I fit y = Alogx + B using polyfit()? The result should be an np.array of [A, B]\nA:\n<code>\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def polyfit(x, y):\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #polynomial fitting\n    #"
    },
    {
        "library_id": 1,
        "source_id": 0,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a set of data and I want to compare which line describes it best (polynomials of different orders, exponential or logarithmic).\nI use Python and Numpy and for polynomial fitting there is a function polyfit(). \nHow do I fit y = A + Blogx using polyfit()? The result should be an np.array of [A, B]\nA:\n<code>\nimport numpy as np\nimport scipy\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def polyfit(x, y, order):\n    \"\"\"\n    Polynomial fitting function."
    },
    {
        "library_id": 2,
        "source_id": 0,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a set of data and I want to compare which line describes it best (polynomials of different orders, exponential or logarithmic).\nI use Python and Numpy and for polynomial fitting there is a function polyfit(). But I found no such functions for exponential and logarithmic fitting.\nHow do I fit y = A*exp(Bx) + C? The result should be an np.array of [A, B, C]. I know that polyfit performs bad for this function, so I would like to use curve_fit to solve the problem, and it should start from initial guess p0.\nA:\n<code>\nimport numpy as np\nimport scipy.optimize\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def func(x, A, B, C):\n    return A*np.exp(B*x) + C"
    },
    {
        "library_id": 3,
        "source_id": 3,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI can't figure out how to do a Two-sample KS test in Scipy.\nAfter reading the documentation scipy kstest\nI can see how to test where a distribution is identical to standard normal distribution\nfrom scipy.stats import kstest\nimport numpy as np\nx = np.random.normal(0,1,1000)\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical, something like:\nfrom scipy.stats import kstest\nimport numpy as np\nx = np.random.normal(0,1,1000)\nz = np.random.normal(1.1,0.9, 1000)\nand test whether x and z are identical\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nA:\n<code>\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(statistic, p_value)\n</code>\n\n\ndef",
        "answer": "def test_statistic(x, y):\n    return stats.kstest(x, 'norm', args=(y,))"
    },
    {
        "library_id": 4,
        "source_id": 3,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI can't figure out how to do a Two-sample KS test in Scipy.\nAfter reading the documentation scipy kstest\nI can see how to test where a distribution is identical to standard normal distribution\nfrom scipy.stats import kstest\nimport numpy as np\nx = np.random.normal(0,1,1000)\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical, something like:\nfrom scipy.stats import kstest\nimport numpy as np\nx = np.random.normal(0,1,1000)\nz = np.random.normal(1.1,0.9, 1000)\nand test whether x and z are identical\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the two distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how should I do it?\nThank You in Advance\nA:\n<code>\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\nalpha = 0.01\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def test_two_sample_KS(x, y, alpha):\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x and y are identical\n    # test whether x"
    },
    {
        "library_id": 5,
        "source_id": 5,
        "prompt": "Complete the Python function given the prompt below:Problem:\nAccording to the SciPy documentation it is possible to minimize functions with multiple variables, yet it doesn't tell how to optimize on such functions.\nfrom scipy.optimize import minimize\nfrom math import *\ndef f(c):\n  return sqrt((sin(pi/2) + sin(0) + sin(c) - 2)**2 + (cos(pi/2) + cos(0) + cos(c) - 1)**2)\nprint minimize(f, 3.14/2 + 3.14/7)\nThe above code does try to minimize the function f, but for my task I need to minimize with respect to three variables, starting from `initial_guess`.\nSimply introducing a second argument and adjusting minimize accordingly yields an error (TypeError: f() takes exactly 2 arguments (1 given)).\nHow does minimize work when minimizing with multiple variables.\nI need to minimize f(a,b,c)=((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4.\nResult should be a list=[a,b,c], the parameters of minimized function.\nA:\n<code>\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def f(c):\n  return sqrt((sin(pi/2) + sin(0) + sin(c) - 2)**2 + (cos(pi/2) + cos(0) + cos(c) - 1)**2)\nprint minimize(f, 3.14/2 + 3.14/7)\nThe above code does try to minimize the function f, but for my task I need to minimize with respect to three variables, starting from `initial_guess`.\nSimply introducing a second argument and adjusting minimize accordingly yields an error (TypeError: f() takes exactly 2 arguments (1 given)).\nHow does minimize work when minimizing with multiple variables.\nI need to minimize f(a,b,c)=((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4.\nResult should be a list=[a,b,c], the parameters of minimized function.\nA:\n<code>\nimport scipy.optimize as optimize\nfrom math import *\ninitial_guess = [-1, 0, -3]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 6,
        "source_id": 6,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow does one convert a list of Z-scores from the Z-distribution (standard normal distribution, Gaussian distribution) to left-tailed p-values? I have yet to find the magical function in Scipy's stats module to do this, but one must be there.\nA:\n<code>\nimport numpy as np\nimport scipy.stats\nz_scores = np.array([-3, -2, 0, 2, 2.5])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(p_values)\n</code>\n\n\ndef",
        "answer": "def p_values(z_scores):\n    \"\"\"\n    Returns the p-values for the given Z-scores.\n    \"\"\"\n    # Calculate the mean and standard deviation of the Z-scores.\n    mean = np.mean(z_scores)\n    std = np.std(z_scores)"
    },
    {
        "library_id": 7,
        "source_id": 6,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow does one convert a list of Z-scores from the Z-distribution (standard normal distribution, Gaussian distribution) to left-tailed p-values? Original data is sampled from X ~ N(mu, sigma). I have yet to find the magical function in Scipy's stats module to do this, but one must be there.\nA:\n<code>\nimport scipy.stats\nimport numpy as np\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(p_values)\n</code>\n\n\ndef",
        "answer": "def p_values(z_scores, mu, sigma):\n    return scipy.stats.norm.sf(z_scores, loc=mu, scale=sigma)"
    },
    {
        "library_id": 8,
        "source_id": 6,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow does one convert a left-tailed p-value to a z_score from the Z-distribution (standard normal distribution, Gaussian distribution)? I have yet to find the magical function in Scipy's stats module to do this, but one must be there.\nA:\n<code>\nimport numpy as np\nimport scipy.stats\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(z_scores)\n</code>\n\n\ndef",
        "answer": "def z_score(p_values):\n    z_scores = []\n    for p_value in p_values:\n        z_score = scipy.stats.norm.ppf(p_value)\n        z_scores.append(z_score)\n    return z_scores"
    },
    {
        "library_id": 9,
        "source_id": 9,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have been trying to get the result of a lognormal distribution using Scipy. I already have the Mu and Sigma, so I don't need to do any other prep work. If I need to be more specific (and I am trying to be with my limited knowledge of stats), I would say that I am looking for the cumulative function (cdf under Scipy). The problem is that I can't figure out how to do this with just the mean and standard deviation on a scale of 0-1 (ie the answer returned should be something from 0-1). I'm also not sure which method from dist, I should be using to get the answer. I've tried reading the documentation and looking through SO, but the relevant questions (like this and this) didn't seem to provide the answers I was looking for.\nHere is a code sample of what I am working with. Thanks. Here mu and stddev stands for mu and sigma in probability density function of lognorm.\nfrom scipy.stats import lognorm\nstddev = 0.859455801705594\nmu = 0.418749176686875\ntotal = 37\ndist = lognorm.cdf(total,mu,stddev)\nUPDATE:\nSo after a bit of work and a little research, I got a little further. But I still am getting the wrong answer. The new code is below. According to R and Excel, the result should be.7434, but that's clearly not what is happening. Is there a logic flaw I am missing?\nstddev = 2.0785\nmu = 1.744\nx = 25\ndist = lognorm([mu],loc=stddev)\ndist.cdf(x)  # yields=0.96374596, expected=0.7434\nA:\n<code>\nimport numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\nx = 25\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def lognorm(mu,stddev,x):\n    return stats.lognorm.cdf(x,mu,stddev)\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 10,
        "source_id": 9,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have been trying to get the arithmetic result of a lognormal distribution using Scipy. I already have the Mu and Sigma, so I don't need to do any other prep work. If I need to be more specific (and I am trying to be with my limited knowledge of stats), I would say that I am looking for the expected value and median of the distribution. The problem is that I can't figure out how to do this with just the mean and standard deviation. I'm also not sure which method from dist, I should be using to get the answer. I've tried reading the documentation and looking through SO, but the relevant questions (like this and this) didn't seem to provide the answers I was looking for.\nHere is a code sample of what I am working with. Thanks. Here mu and stddev stands for mu and sigma in probability density function of lognorm.\nfrom scipy.stats import lognorm\nstddev = 0.859455801705594\nmu = 0.418749176686875\ntotal = 37\ndist = lognorm(total,mu,stddev)\nWhat should I do next?\nA:\n<code>\nimport numpy as np\nfrom scipy import stats\nstddev = 2.0785\nmu = 1.744\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(expected_value, median)\n</code>\n\n\ndef",
        "answer": "def expected_value(total, mu, stddev):\n    return (total * np.sqrt(2 * np.pi * stddev ** 2)) / mu"
    },
    {
        "library_id": 11,
        "source_id": 11,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def matrix_multiply(m,c):\n    return np.dot(m,c)"
    },
    {
        "library_id": 12,
        "source_id": 11,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def f(sA = example_sA, sB = example_sB):\n    return np.dot(sA,sB)\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 13,
        "source_id": 13,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have some data that comes in the form (x, y, z, V) where x,y,z are distances, and V is the moisture. I read a lot on StackOverflow about interpolation by python like this and this valuable posts, but all of them were about regular grids of x, y, z. i.e. every value of x contributes equally with every point of y, and every point of z. On the other hand, my points came from 3D finite element grid (as below), where the grid is not regular. \nThe two mentioned posts 1 and 2, defined each of x, y, z as a separate numpy array then they used something like cartcoord = zip(x, y) then scipy.interpolate.LinearNDInterpolator(cartcoord, z) (in a 3D example). I can not do the same as my 3D grid is not regular, thus not each point has a contribution to other points, so if when I repeated these approaches I found many null values, and I got many errors.\nHere are 10 sample points in the form of [x, y, z, V]\ndata = [[27.827, 18.530, -30.417, 0.205], [24.002, 17.759, -24.782, 0.197], \n[22.145, 13.687, -33.282, 0.204], [17.627, 18.224, -25.197, 0.197], \n[29.018, 18.841, -38.761, 0.212], [24.834, 20.538, -33.012, 0.208], \n[26.232, 22.327, -27.735, 0.204], [23.017, 23.037, -29.230, 0.205], \n[28.761, 21.565, -31.586, 0.211], [26.263, 23.686, -32.766, 0.215]]\nI want to get the interpolated value V of the point (25, 20, -30).\nHow can I get it?\nA:\n<code>\nimport numpy as np\nimport scipy.interpolate\npoints = np.array([\n        [ 27.827,  18.53, -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def interpolate(x, y, z, V):\n    return scipy.interpolate.LinearNDInterpolator(cartcoord, z)"
    },
    {
        "library_id": 14,
        "source_id": 13,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have some data that comes in the form (x, y, z, V) where x,y,z are distances, and V is the moisture. I read a lot on StackOverflow about interpolation by python like this and this valuable posts, but all of them were about regular grids of x, y, z. i.e. every value of x contributes equally with every point of y, and every point of z. On the other hand, my points came from 3D finite element grid (as below), where the grid is not regular. \nThe two mentioned posts 1 and 2, defined each of x, y, z as a separate numpy array then they used something like cartcoord = zip(x, y) then scipy.interpolate.LinearNDInterpolator(cartcoord, z) (in a 3D example). I can not do the same as my 3D grid is not regular, thus not each point has a contribution to other points, so if when I repeated these approaches I found many null values, and I got many errors.\nHere are 10 sample points in the form of [x, y, z, V]\ndata = [[27.827, 18.530, -30.417, 0.205], [24.002, 17.759, -24.782, 0.197], \n[22.145, 13.687, -33.282, 0.204], [17.627, 18.224, -25.197, 0.197], \n[29.018, 18.841, -38.761, 0.212], [24.834, 20.538, -33.012, 0.208], \n[26.232, 22.327, -27.735, 0.204], [23.017, 23.037, -29.230, 0.205], \n[28.761, 21.565, -31.586, 0.211], [26.263, 23.686, -32.766, 0.215]]\nI want to get the interpolated value V of the point (25, 20, -30) and (27, 20, -32) as a list.\nHow can I get it?\nA:\n<code>\nimport numpy as np\nimport scipy.interpolate\npoints = np.array([\n        [ 27.827,  18.53, -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def interpolate(x, y, z, V):\n    return scipy.interpolate.LinearNDInterpolator(cartcoord, z)"
    },
    {
        "library_id": 15,
        "source_id": 15,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a numpy array for an image that I read in from a FITS file. I rotated it by N degrees using scipy.ndimage.interpolation.rotate. Then I want to figure out where some point (x,y) in the original non-rotated frame ends up in the rotated image -- i.e., what are the rotated frame coordinates (x',y')?\nThis should be a very simple rotation matrix problem but if I do the usual mathematical or programming based rotation equations, the new (x',y') do not end up where they originally were. I suspect this has something to do with needing a translation matrix as well because the scipy rotate function is based on the origin (0,0) rather than the actual center of the image array.\nCan someone please tell me how to get the rotated frame (x',y')? As an example, you could use\nfrom scipy import misc\nfrom scipy.ndimage import rotate\ndata_orig = misc.face()\ndata_rot = rotate(data_orig,66) # data array\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nA:\n<code>\nfrom scipy import misc\nfrom scipy.ndimage import rotate\nimport numpy as np\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(data_rot, (xrot, yrot))\n</code>\n\n\ndef",
        "answer": "def rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by angle (in degrees)\n    \"\"\"\n    # grab the dimensions of the image\n    (h, w) = image.shape[:2]"
    },
    {
        "library_id": 16,
        "source_id": 16,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow can I extract the main diagonal(1-d array) of a sparse matrix? The matrix is created in scipy.sparse. I want equivalent of np.diagonal(), but for sparse matrix.\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def main():\n    import doctest\n    doctest.testmod()"
    },
    {
        "library_id": 17,
        "source_id": 17,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI simulate times in the range 0 to T according to a Poisson process. The inter-event times are exponential and we know that the distribution of the times should be uniform in the range 0 to T.\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nI would simply like to run one of the tests for uniformity, for example the Kolmogorov-Smirnov test. I can't work out how to do this in scipy however. If I do\nimport random\nfrom scipy.stats import kstest\ntimes = poisson_simul(1, 100)\nprint kstest(times, \"uniform\") \nit is not right. It gives me\n(1.0, 0.0)\nI just want to test the hypothesis that the points are uniformly chosen from the range 0 to T. How do you do this in scipy? The result should be KStest result.\nA:\n<code>\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nI would like to run one of the tests for uniformity, for example the Kolmogorov-Smirnov test. I can't work out how to do this in scipy however. If I do\nimport random\nfrom scipy.stats import kstest\ntimes = poisson_simul(1, 100)\nprint kstest(times, \"uniform\") \nit is not right. It gives me\n(1.0, 0.0)\nI just want to test the hypothesis that the points are uniformly chosen from the range 0 to T. How do you do this in scipy? The result should be KStest result.\nA:\n<code>\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 18,
        "source_id": 17,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI simulate times in the range 0 to T according to a Poisson process. The inter-event times are exponential and we know that the distribution of the times should be uniform in the range 0 to T.\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nI would simply like to run one of the tests for uniformity, for example the Kolmogorov-Smirnov test. I can't work out how to do this in scipy however. If I do\nimport random\nfrom scipy.stats import kstest\ntimes = poisson_simul(1, 100)\nprint kstest(times, \"uniform\") \nit is not right. It gives me\n(1.0, 0.0)\nI just want to test the hypothesis that the points are uniformly chosen from the range 0 to T. How do you do this in scipy? The result should be KStest result.\nA:\n<code>\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nexample_rate = 1.0\nexample_T = 100.0\nexample_times = poisson_simul(example_rate, example_T)\ndef f(times = example_times, rate = example_rate, T = example_T):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nI would like to run one of the tests for uniformity, for example the Kolmogorov-Smirnov test. I can't work out how to do this in scipy however. If I do\nimport random\nfrom scipy.stats import kstest\ntimes = poisson_simul(1, 100)\nprint kstest(times, \"uniform\") \nit is not right. It gives me\n(1.0, 0.0)\nI just want to test the hypothesis that the points are uniformly chosen from the range 0 to T. How do you do this in scipy? The result should be KStest result.\nA:\n<code>\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nexample_rate = 1.0\nexample_T = 100.0\nexample_times = poisson_simul(example_rate, example_T)\ndef f(times = example_times, rate = example_rate, T = example_T):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>"
    },
    {
        "library_id": 19,
        "source_id": 17,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI simulate times in the range 0 to T according to a Poisson process. The inter-event times are exponential and we know that the distribution of the times should be uniform in the range 0 to T.\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nI would simply like to run one of the tests for uniformity, for example the Kolmogorov-Smirnov test. I can't work out how to do this in scipy however. If I do\nimport random\nfrom scipy.stats import kstest\ntimes = poisson_simul(1, 100)\nprint kstest(times, \"uniform\") \nit is not right. It gives me\n(1.0, 0.0)\nI just want to test the hypothesis that the points are uniformly chosen from the range 0 to T. How do you do this in scipy? Another question is how to interpret the result? What I want is just `True` for unifomity or `False` vice versa. Suppose I want a confidence level of 95%.\nA:\n<code>\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nI would like to run one of the tests for uniformity, for example the Kolmogorov-Smirnov test. I can't work out how to do this in scipy however. If I do\nimport random\nfrom scipy.stats import kstest\ntimes = poisson_simul(1, 100)\nprint kstest(times, \"uniform\") \nit is not right. It gives me\n(1.0, 0.0)\nI just want to test the hypothesis that the points are uniformly chosen from the range 0 to T. How do you do this in scipy? Another question is how to interpret the result? What I want is just `True` for unifomity or `False` vice versa. Suppose I want a confidence level of 95%.\nA:\n<code>\nfrom scipy import stats\nimport random\nimport numpy as np\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 20,
        "source_id": 20,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have two csr_matrix, c1, c2.\nI want a new matrix Feature = [c1, c2]. But if I directly concatenate them horizontally this way, there's an error that says the matrix Feature is a list. How can I achieve the matrix concatenation and still get the same type of matrix, i.e. a csr_matrix?\nAnd it doesn't work if I do this after the concatenation: Feature = csr_matrix(Feature) It gives the error:\nTraceback (most recent call last):\n  File \"yelpfilter.py\", line 91, in <module>\n    Feature = csr_matrix(Feature)\n  File \"c:\\python27\\lib\\site-packages\\scipy\\sparse\\compressed.py\", line 66, in __init__\n    self._set_self( self.__class__(coo_matrix(arg1, dtype=dtype)) )\n  File \"c:\\python27\\lib\\site-packages\\scipy\\sparse\\coo.py\", line 185, in __init__\n    self.row, self.col = M.nonzero()\nTypeError: __nonzero__ should return bool or int, returned numpy.bool_\nA:\n<code>\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n#print(Feature)\n</code>\n\n\ndef",
        "answer": "def Feature(Feature):\n    Feature = Feature.tolist()\n    Feature = [Feature[0], Feature[1]]\n    return Feature"
    },
    {
        "library_id": 21,
        "source_id": 20,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have two csr_matrix, c1 and c2.\nI want a new sparse matrix Feature = [c1, c2], that is, to stack c1 and c2 horizontally to get a new sparse matrix.\nTo make use of sparse matrix's memory efficiency, I don't want results as dense arrays.\nBut if I directly concatenate them this way, there's an error that says the matrix Feature is a list.\nAnd if I try this: Feature = csr_matrix(Feature) It gives the error:\nTraceback (most recent call last):\n  File \"yelpfilter.py\", line 91, in <module>\n    Feature = csr_matrix(Feature)\n  File \"c:\\python27\\lib\\site-packages\\scipy\\sparse\\compressed.py\", line 66, in __init__\n    self._set_self( self.__class__(coo_matrix(arg1, dtype=dtype)) )\n  File \"c:\\python27\\lib\\site-packages\\scipy\\sparse\\coo.py\", line 185, in __init__\n    self.row, self.col = M.nonzero()\nTypeError: __nonzero__ should return bool or int, returned numpy.bool_\nAny help would be appreciated!\nA:\n<code>\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n#print(Feature)\n</code>\n\n\ndef",
        "answer": "def Feature(arg1):\n    return csr_matrix(arg1)"
    },
    {
        "library_id": 22,
        "source_id": 20,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have two csr_matrix, c1 and c2.\nI want a new matrix \nFeature = [c1\n           c2]. \n          \nThat is, I want to concatenate c1 and c2 in vertical direction.          \nBut I don't know how to represent the concatenation or how to form the format.\nHow can I achieve the matrix concatenation and still get the same type of matrix, i.e. a csr_matrix?\nAny help would be appreciated.\nA:\n<code>\nfrom scipy import sparse\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n#print(Feature)\n</code>\n\n\ndef",
        "answer": "def concatenate(c1, c2):\n    return sparse.vstack([c1, c2])"
    },
    {
        "library_id": 23,
        "source_id": 23,
        "prompt": "Complete the Python function given the prompt below:Problem:\nGiven two sets of points in n-dimensional space, how can one map points from one set to the other, such that each point is only used once and the total euclidean distance between the pairs of points is minimized?\nFor example,\nimport matplotlib.pyplot as plt\nimport numpy as np\n# create six points in 2d space; the first three belong to set \"A\" and the\n# second three belong to set \"B\"\nx = [1, 2, 3, 1.8, 1.9, 3.4]\ny = [2, 3, 1, 2.6, 3.4, 0.4]\ncolors = ['red'] * 3 + ['blue'] * 3\nplt.scatter(x, y, c=colors)\nplt.show()\nSo in the example above, the goal would be to map each red point to a blue point such that each blue point is only used once and the sum of the distances between points is minimized.\nThe application I have in mind involves a fairly small number of datapoints in 3-dimensional space, so the brute force approach might be fine, but I thought I would check to see if anyone knows of a more efficient or elegant solution first. \nThe result should be an assignment of points in second set to corresponding elements in the first set.\nFor example, a matching solution is\nPoints1 <-> Points2\n    0   ---     2\n    1   ---     0\n    2   ---     1\nand the result is [2, 0, 1]\nA:\n<code>\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def distance(p1, p2):\n    return np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)"
    },
    {
        "library_id": 24,
        "source_id": 23,
        "prompt": "Complete the Python function given the prompt below:Problem:\nGiven two sets of points in n-dimensional space, how can one map points from one set to the other, such that each point is only used once and the total Manhattan distance between the pairs of points is minimized?\nFor example,\nimport matplotlib.pyplot as plt\nimport numpy as np\n# create six points in 2d space; the first three belong to set \"A\" and the\n# second three belong to set \"B\"\nx = [1, 2, 3, 1.8, 1.9, 3.4]\ny = [2, 3, 1, 2.6, 3.4, 0.4]\ncolors = ['red'] * 3 + ['blue'] * 3\nplt.scatter(x, y, c=colors)\nplt.show()\nSo in the example above, the goal would be to map each red point to a blue point such that each blue point is only used once and the sum of the distances between points is minimized.\nThe application I have in mind involves a fairly small number of datapoints in 3-dimensional space, so the brute force approach might be fine, but I thought I would check to see if anyone knows of a more efficient or elegant solution first.\nThe result should be an assignment of points in second set to corresponding elements in the first set.\nFor example, a matching solution is\nPoints1 <-> Points2\n    0   ---     2\n    1   ---     0\n    2   ---     1\nand the result is [2, 0, 1]\nA:\n<code>\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def distance(p1, p2):\n    return np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)"
    },
    {
        "library_id": 25,
        "source_id": 25,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, these elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\nIf I try it using lil_matrix, it works:\n>>> a = np.ones((2,2))\n>>> c = lil_matrix(a)\n>>> c.setdiag(0)\n>>> c\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in LInked List format>\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n>>> b = csr_matrix(a)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any workaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\n\n\ndef",
        "answer": "def test_setdiag():\n    a = np.ones((2, 2))\n    b = lil_matrix(a)\n    c = b.copy()\n    c.setdiag(0)\n    c.setdiag(0, k=1)\n    c.setdiag(0, k=-1)\n    c.setdiag(0, k=-2)\n    c.setdiag(0, k=-3)\n    c.setdiag(0, k=-4)\n    c.setdiag(0, k=-5)\n    c.setdiag(0, k=-6)\n    c.setdiag(0, k=-7)\n    c.setdiag(0, k=-8)\n    c.setdiag(0, k=-9)\n    c.setdiag(0, k=-10)\n    c.setdiag(0, k=-11)\n    c.setdiag(0, k=-12)\n    c.setdiag(0, k=-13)\n    c.setdiag(0, k=-14)\n    c.setdiag(0, k=-15)\n    c.setdiag(0, k=-16)\n    c.setdiag(0, k=-17)\n    c.setdiag(0, k=-18)\n    c.setdiag(0, k=-19)\n    c.setdiag(0, k=-20)\n    c.setdiag(0, k=-21)\n    c.setdiag(0, k=-22)\n    c.setdiag(0, k=-23)\n    c.setdiag(0, k=-24)\n    c.setdiag(0, k=-25)\n    c.setdiag(0, k=-26)\n    c.setdiag(0, k=-27)\n    c.setdiag(0, k=-28)\n    c.setdiag(0, k=-29)\n    c.setdiag(0, k=-30)\n    c.setdiag(0, k=-31)\n    c.set"
    },
    {
        "library_id": 26,
        "source_id": 26,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am working with a 2D numpy array made of 512x512=262144 values. Such values are of float type and range from 0.0 to 1.0. The array has an X,Y coordinate system which originates in the top left corner: thus, position (0,0) is in the top left corner, while position (512,512) is in the bottom right corner.\nThis is how the 2D array looks like (just an excerpt):\nX,Y,Value\n0,0,0.482\n0,1,0.49\n0,2,0.496\n0,3,0.495\n0,4,0.49\n0,5,0.489\n0,6,0.5\n0,7,0.504\n0,8,0.494\n0,9,0.485\nI would like to be able to:\nCount the number of regions of cells which value exceeds a given threshold, i.e. 0.75;\nNote: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\nA:\n<code>\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def count_regions(img, threshold):\n    result = 0\n    for x in range(img.shape[0]):\n        for y in range(img.shape[1]):\n            if img[x, y] > threshold:\n                result += 1\n    return result"
    },
    {
        "library_id": 27,
        "source_id": 26,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am working with a 2D numpy array made of 512x512=262144 values. Such values are of float type and range from 0.0 to 1.0. The array has an X,Y coordinate system which originates in the top left corner: thus, position (0,0) is in the top left corner, while position (512,512) is in the bottom right corner.\nThis is how the 2D array looks like (just an excerpt):\nX,Y,Value\n0,0,0.482\n0,1,0.49\n0,2,0.496\n0,3,0.495\n0,4,0.49\n0,5,0.489\n0,6,0.5\n0,7,0.504\n0,8,0.494\n0,9,0.485\nI would like to be able to:\nCount the number of regions of cells which value below a given threshold, i.e. 0.75;\nNote: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\nA:\n<code>\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def count_regions(img, threshold):\n    result = 0\n    for x in range(img.shape[0]):\n        for y in range(img.shape[1]):\n            if img[x, y] < threshold:\n                result += 1\n    return result"
    },
    {
        "library_id": 28,
        "source_id": 26,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am working with a 2D numpy array made of 512x512=262144 values. Such values are of float type and range from 0.0 to 1.0. The array has an X,Y coordinate system which originates in the top left corner: thus, position (0,0) is in the top left corner, while position (512,512) is in the bottom right corner.\nThis is how the 2D array looks like (just an excerpt):\nX,Y,Value\n0,0,0.482\n0,1,0.49\n0,2,0.496\n0,3,0.495\n0,4,0.49\n0,5,0.489\n0,6,0.5\n0,7,0.504\n0,8,0.494\n0,9,0.485\nI would like to be able to:\nCount the number of regions of cells which value exceeds a given threshold, i.e. 0.75;\nNote: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\nA:\n<code>\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nexample_img /= img.max()\ndef f(img = example_img):\n    threshold = 0.75\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def f(img = example_img):\n    threshold = 0.75\n    result = np.zeros(img.shape, dtype=np.double)\n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            if img[i, j] > threshold:\n                result[i, j] = 1\n    return result\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 29,
        "source_id": 26,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am working with a 2D numpy array made of 512x512=262144 values. Such values are of float type and range from 0.0 to 1.0. The array has an X,Y coordinate system which originates in the top left corner: thus, position (0,0) is in the top left corner, while position (512,512) is in the bottom right corner.\nThis is how the 2D array looks like (just an excerpt):\nX,Y,Value\n0,0,0.482\n0,1,0.49\n0,2,0.496\n0,3,0.495\n0,4,0.49\n0,5,0.489\n0,6,0.5\n0,7,0.504\n0,8,0.494\n0,9,0.485\nI would like to be able to:\nFind the regions of cells which value exceeds a given threshold, say 0.75;\nNote: If two elements touch horizontally, vertically or diagnoally, they belong to one region.\nDetermine the distance between the center of mass of such regions and the top left corner, which has coordinates (0,0).\nPlease output the distances as a list.\nA:\n<code>\nimport numpy as np\nfrom scipy import ndimage\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def find_regions(img, threshold):\n    result = []\n    for x in range(img.shape[0]):\n        for y in range(img.shape[1]):\n            if img[x, y] > threshold:\n                result.append((x, y))\n    return result"
    },
    {
        "library_id": 30,
        "source_id": 30,
        "prompt": "Complete the Python function given the prompt below:Problem:\nIs there a simple and efficient way to make a sparse scipy matrix (e.g. lil_matrix, or csr_matrix) symmetric? \nCurrently I have a lil sparse matrix, and not both of sA[i,j] and sA[j,i] have element for any i,j.\nWhen populating a large sparse co-occurrence matrix it would be highly inefficient to fill in [row, col] and [col, row] at the same time. What I'd like to be doing is:\nfor i in data:\n    for j in data:\n        if have_element(i, j):\n            lil_sparse_matrix[i, j] = some_value\n            # want to avoid this:\n            # lil_sparse_matrix[j, i] = some_value\n# this is what I'm looking for:\nlil_sparse.make_symmetric() \nand it let sA[i,j] = sA[j,i] for any i, j.\nThis is similar to <a href=\"https://stackoverflow.com/questions/2572916/numpy-smart-symmetric-matrix\">stackoverflow's numpy-smart-symmetric-matrix question, but is particularly for scipy sparse matrices.\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy import sparse\nM= sparse.random(10, 10, density=0.1, format='lil')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(M)\n</code>\n\n\ndef",
        "answer": "def have_element(i, j):\n    return i in M[j]"
    },
    {
        "library_id": 31,
        "source_id": 30,
        "prompt": "Complete the Python function given the prompt below:Problem:\nIs there a simple and efficient way to make a sparse scipy matrix (e.g. lil_matrix, or csr_matrix) symmetric? \nCurrently I have a lil sparse matrix, and not both of sA[i,j] and sA[j,i] have element for any i,j.\nWhen populating a large sparse co-occurrence matrix it would be highly inefficient to fill in [row, col] and [col, row] at the same time. What I'd like to be doing is:\nfor i in data:\n    for j in data:\n        if have_element(i, j):\n            lil_sparse_matrix[i, j] = some_value\n            # want to avoid this:\n            # lil_sparse_matrix[j, i] = some_value\n# this is what I'm looking for:\nlil_sparse.make_symmetric() \nand it let sA[i,j] = sA[j,i] for any i, j.\nThis is similar to <a href=\"https://stackoverflow.com/questions/2572916/numpy-smart-symmetric-matrix\">stackoverflow's numpy-smart-symmetric-matrix question, but is particularly for scipy sparse matrices.\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nexample_sA = sparse.random(10, 10, density=0.1, format='lil')\ndef f(sA = example_sA):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return sA\n</code>\n\n\ndef",
        "answer": "def f(sA = example_sA):\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 32,
        "source_id": 32,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to reduce noise in a binary python array by removing all completely isolated single cells, i.e. setting \"1\" value cells to 0 if they are completely surrounded by other \"0\"s like this:\n0 0 0\n0 1 0\n0 0 0\n I have been able to get a working solution by removing blobs with sizes equal to 1 using a loop, but this seems like a very inefficient solution for large arrays.\nIn this case, eroding and dilating my array won't work as it will also remove features with a width of 1. I feel the solution lies somewhere within the scipy.ndimage package, but so far I haven't been able to crack it. Any help would be greatly appreciated!\nA:\n<code>\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(square)\n</code>\n\n\ndef",
        "answer": "def remove_isolated_cells(array):\n    \"\"\"\n    Removes all isolated cells from an array.\n    \"\"\"\n    return np.where(np.all(array == 0, axis=0), 1, array)"
    },
    {
        "library_id": 33,
        "source_id": 32,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to reduce noise in a python image array by removing all completely isolated single cells, i.e. setting nonzero value cells to 0 if they are completely surrounded by other \"0\"s like this:\n0 0 0\n0 8 0\n0 0 0\n I have been able to get a working solution by removing blobs with sizes equal to 1 using a loop, but this seems like a very inefficient solution for large arrays.\nIn this case, eroding and dilating my array won't work as it will also remove features with a width of 1. I feel the solution lies somewhere within the scipy.ndimage package, but so far I haven't been able to crack it. Any help would be greatly appreciated!\nA:\n<code>\nimport numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = np.random.randint(1, 255, size = (12, 12))\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = np.random.randint(1, 255, size = (20,))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(square)\n</code>\n\n\ndef",
        "answer": "def remove_blobs(image):\n    # Remove blobs with size equal to 1\n    blobs = scipy.ndimage.find_objects(image)\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1] == 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs = [blob for blob in blobs if blob[1]!= 1]\n    # Remove blobs with size equal to 1\n    blobs ="
    },
    {
        "library_id": 34,
        "source_id": 34,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a sparse 988x1 vector (stored in col, a column in a csr_matrix) created through scipy.sparse. Is there a way to gets its mean and standard deviation without having to convert the sparse matrix to a dense one?\nnumpy.mean seems to only work for dense vectors.\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(mean)\nprint(standard_deviation)\n</code>\n\n\ndef",
        "answer": "def mean(arr):\n    return np.mean(arr)"
    },
    {
        "library_id": 35,
        "source_id": 34,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a sparse 988x1 vector (stored in col, a column in a csr_matrix) created through scipy.sparse. Is there a way to gets its max and min value without having to convert the sparse matrix to a dense one?\nnumpy.max seems to only work for dense vectors.\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(Max)\nprint(Min)\n</code>\n\n\ndef",
        "answer": "def maxmin(arr):\n    return np.max(arr),np.min(arr)"
    },
    {
        "library_id": 36,
        "source_id": 34,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a sparse 988x1 vector (stored in col, a column in a csr_matrix) created through scipy.sparse. Is there a way to gets its median and mode value without having to convert the sparse matrix to a dense one?\nnumpy.median seems to only work for dense vectors.\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(Median)\nprint(Mode)\n</code>\n\n\ndef",
        "answer": "def get_median(arr):\n    return np.median(arr)"
    },
    {
        "library_id": 37,
        "source_id": 37,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'd like to achieve a fourier series development for a x-y-dataset using numpy and scipy.\nAt first I want to fit my data with the first 8 cosines and plot additionally only the first harmonic. So I wrote the following two function defintions:\n# fourier series defintions\ntau = 0.045\ndef fourier8(x, a1, a2, a3, a4, a5, a6, a7, a8):\n    return a1 * np.cos(1 * np.pi / tau * x) + \\\n           a2 * np.cos(2 * np.pi / tau * x) + \\\n           a3 * np.cos(3 * np.pi / tau * x) + \\\n           a4 * np.cos(4 * np.pi / tau * x) + \\\n           a5 * np.cos(5 * np.pi / tau * x) + \\\n           a6 * np.cos(6 * np.pi / tau * x) + \\\n           a7 * np.cos(7 * np.pi / tau * x) + \\\n           a8 * np.cos(8 * np.pi / tau * x)\ndef fourier1(x, a1):\n    return a1 * np.cos(1 * np.pi / tau * x)\nThen I use them to fit my data:\n# import and filename\nfilename = 'data.txt'\nimport numpy as np\nfrom scipy.optimize import curve_fit\nz, Ua = np.loadtxt(filename,delimiter=',', unpack=True)\ntau = 0.045\npopt, pcov = curve_fit(fourier8, z, Ua)\nwhich works as desired\nBut know I got stuck making it generic for arbitary orders of harmonics, e.g. I want to fit my data with the first fifteen harmonics.\nHow could I achieve that without defining fourier1, fourier2, fourier3..., fourier15?\nBy the way, initial guess of a1,a2,\u2026 should be set to default value.\nA:\n<code>\nfrom scipy.optimize import curve_fit\nimport numpy as np\ns = '''1.000000000000000021e-03,2.794682735905079767e+02\n4.000000000000000083e-03,2.757183469104809888e+02\n1.400000000000000029e-02,2.791403179603880176e+02\n2.099999999999999784e-02,1.781413355804160119e+02\n3.300000000000000155e-02,-2.798375517344049968e+02\n4.199999999999999567e-02,-2.770513900380149721e+02\n5.100000000000000366e-02,-2.713769422793179729e+02\n6.900000000000000577e-02,1.280740698304900036e+02\n7.799999999999999989e-02,2.800801708984579932e+02\n8.999999999999999667e-02,2.790400329037249776e+02'''.replace('\\n', ';')\narr = np.matrix(s)\nz = np.array(arr[:, 0]).squeeze()\nUa = np.array(arr[:, 1]).squeeze()\ntau = 0.045\ndegree = 15    \n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(popt, pcov)\n</code>\n\n\ndef",
        "answer": "def fourier1(x, a1):\n    return a1 * np.cos(1 * np.pi / tau * x)"
    },
    {
        "library_id": 38,
        "source_id": 38,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a raster with a set of unique ID patches/regions which I've converted into a two-dimensional Python numpy array. I would like to calculate pairwise Euclidean distances between all regions to obtain the minimum distance separating the nearest edges of each raster patch. As the array was originally a raster, a solution needs to account for diagonal distances across cells (I can always convert any distances measured in cells back to metres by multiplying by the raster resolution).\nI've experimented with the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\nHere's a sample dataset resembling my input data:\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Sample study area array\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n# Plot array\nplt.imshow(example_array, cmap=\"spectral\", interpolation='nearest')\nA:\n<code>\nimport numpy as np\nimport scipy.spatial.distance\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def cdist(x, y):\n    return scipy.spatial.distance.cdist(x, y, 'euclidean')"
    },
    {
        "library_id": 39,
        "source_id": 38,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a raster with a set of unique ID patches/regions which I've converted into a two-dimensional Python numpy array. I would like to calculate pairwise Manhattan distances between all regions to obtain the minimum distance separating the nearest edges of each raster patch.\nI've experimented with the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\nHere's a sample dataset resembling my input data:\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Sample study area array\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n# Plot array\nplt.imshow(example_array, cmap=\"spectral\", interpolation='nearest')\nA:\n<code>\nimport numpy as np\nimport scipy.spatial.distance\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def cdist(x, y):\n    return scipy.spatial.distance.cdist(x, y, 'cityblock')"
    },
    {
        "library_id": 40,
        "source_id": 38,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a raster with a set of unique ID patches/regions which I've converted into a two-dimensional Python numpy array. I would like to calculate pairwise Euclidean distances between all regions to obtain the minimum distance separating the nearest edges of each raster patch. As the array was originally a raster, a solution needs to account for diagonal distances across cells (I can always convert any distances measured in cells back to metres by multiplying by the raster resolution).\nI've experimented with the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\nHere's a sample dataset resembling my input data:\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Sample study area array\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n# Plot array\nplt.imshow(example_array, cmap=\"spectral\", interpolation='nearest')\nA:\n<code>\nimport numpy as np\nimport scipy.spatial.distance\nexample_arr = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\ndef f(example_array = example_arr):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def cdist(example_array = example_arr):\n    # Calculate pairwise Euclidean distances between all regions to obtain the minimum distance separating the nearest edges of each raster patch. As the array was originally a raster, a solution needs to account for diagonal distances across cells (I can always convert any distances measured in cells back to metres by multiplying by the raster resolution).\n    # Note: I've been using the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\n    # Note: I've been using the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\n    # Note: I've been using the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\n    # Note: I've been using the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\n    # Note: I've been using the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a N*N array in the form of \"from ID, to ID, distance\", including distances between all possible combinations of regions.\n    # Note:"
    },
    {
        "library_id": 41,
        "source_id": 41,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am able to interpolate the data points (dotted lines), and am looking to extrapolate them in both direction.\nHow can I extrapolate these curves in Python with NumPy/SciPy?\nThe code I used for the interpolation is given below,\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import interpolate\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nplt.figure(figsize = (5.15,5.15))\nplt.subplot(111)\nfor i in range(5):\n    x_val = np.linspace(x[0, i], x[-1, i], 100)\n    x_int = np.interp(x_val, x[:, i], y[:, i])\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    plt.plot(x[:, i], y[:, i], linestyle = '', marker = 'o')\n    plt.plot(x_val, y_int, linestyle = ':', linewidth = 0.25, color =  'black')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show() \nThat seems only work for interpolation.\nI want to use B-spline (with the same parameters setting as in the code) in scipy to do extrapolation. The result should be (5, 100) array containing f(x_val) for each group of x, y(just as shown in the code).\nA:\n<code>\nfrom scipy import interpolate\nimport numpy as np\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85.36, 93.24],\n              [61.48, 69.31, 75.36, 81.87, 89.35],\n              [57.61, 65.75, 71.7, 79.1, 86.13],\n              [55.12, 63.34, 69.32, 77.29, 83.88],\n              [54.58, 62.54, 68.7, 76.72, 82.92],\n              [56.58, 63.87, 70.3, 77.69, 83.53],\n              [61.67, 67.79, 74.41, 80.43, 85.86],\n              [70.08, 74.62, 80.93, 85.06, 89.84]])\nx_val = np.linspace(-1, 1, 100)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def Bspline(x, y):\n    x_val = np.linspace(-1, 1, 100)\n    y_int = np.interp(x_val, x, y)\n    tck = interpolate.splrep(x, y, k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    return y_int"
    },
    {
        "library_id": 42,
        "source_id": 42,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow do we pass four datasets in scipy.stats.anderson_ksamp?\nThe anderson function asks only for one parameter and that should be 1-d array. So I am wondering how to pass four different arrays to be compared in it? Thanks\nA:\n<code>\nimport numpy as np\nimport scipy.stats as ss\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\nx3=[34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0]\nx4=[34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(statistic, critical_values, significance_level)\n</code>\n\n\ndef",
        "answer": "def anderson_ksamp(x1, x2, x3, x4):\n    x1_mean = np.mean(x1)\n    x2_mean = np.mean(x2)\n    x3_mean = np.mean(x3)\n    x4_mean = np.mean(x4)\n    x1_std = np.std(x1)\n    x2_std = np.std(x2)\n    x3_std = np.std(x3)\n    x4_std = np.std(x4)\n    x1_var = np.var(x1)\n    x2_var = np.var(x2)\n    x3_var = np.var(x3)\n    x4_var = np.var(x4)\n    x1_kurt = ss.kurtosis(x1)\n    x2_kurt = ss.kurtosis(x2)\n    x3_kurt = ss.kurtosis(x3)\n    x4_kurt = ss.kurtosis(x4)\n    x1_skew = ss.skew(x1)\n    x2_skew = ss.skew(x2)\n    x3_skew = ss.skew(x3)\n    x4_skew = ss.skew(x4)\n    x1_kurt_skew = ss.kurtosis(x1, fisher=False)\n    x2_kurt_skew = ss.kurtosis(x2, fisher=False)\n    x3_kurt_skew = ss.kurtosis(x3, fisher=False)\n    x4_kurt_skew = ss.kurtosis(x4, fisher=False)\n    x1_skew_skew = ss.skew(x1, fisher=False)\n    x2_skew_skew = ss.skew(x2, fisher=False)\n    x3_skew_skew = ss.skew(x3,"
    },
    {
        "library_id": 43,
        "source_id": 42,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow do we pass two datasets in scipy.stats.anderson_ksamp?\nThe anderson function asks only for one parameter and that should be 1-d array. So I am wondering how to pass two different arrays to be compared in it? \nFurther, I want to interpret the result, that is, telling whether the two different arrays are drawn from the same population at the 5% significance level, result should be `True` or `False`. \nA:\n<code>\nimport numpy as np\nimport scipy.stats as ss\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def anderson_ksamp(x1, x2):\n    # x1 and x2 are two arrays\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are the same length\n    # x1 and x2 are"
    },
    {
        "library_id": 44,
        "source_id": 44,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to use rollapply with a formula that requires 2 arguments. To my knowledge the only way (unless you create the formula from scratch) to calculate kendall tau correlation, with standard tie correction included is:\n>>> import scipy\n>>> x = [5.05, 6.75, 3.21, 2.66]\n>>> y = [1.65, 26.5, -5.93, 7.96]\n>>> z = [1.65, 2.64, 2.64, 6.95]\n>>> print scipy.stats.stats.kendalltau(x, y)[0]\n0.333333333333\nI'm also aware of the problem with rollapply and taking two arguments, as documented here:\n\u2022    Related Question 1\n\u2022    Github Issue\n\u2022    Related Question 2\nStill, I'm struggling to find a way to do the kendalltau calculation on a dataframe with multiple columns on a rolling basis.\nMy dataframe is something like this\nA = pd.DataFrame([[1, 5, 1], [2, 4, 1], [3, 3, 1], [4, 2, 1], [5, 1, 1]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\nTrying to create a function that does this\nIn [1]:function(A, 3)  # A is df, 3 is the rolling window\nOut[2]:\n   A  B  C     AB     AC     BC  \n1  1  5  2    NaN    NaN    NaN\n2  2  4  4    NaN    NaN    NaN\n3  3  3  1  -1.00  -0.333   0.333\n4  4  2  2  -1.00  -0.333   0.333\n5  5  1  4  -1.00   1.00  -1.00\nIn a very preliminary approach I entertained the idea of defining the function like this:\ndef tau1(x):\n    y = np.array(A['A']) #  keep one column fix and run it in the other two\n    tau, p_value = sp.stats.kendalltau(x, y)\n    return tau\n A['AB'] = pd.rolling_apply(A['B'], 3, lambda x: tau1(x))\nOff course It didn't work. I got:\nValueError: all keys need to be the same shape\nI understand is not a trivial problem. I appreciate any input.\nA:\n<code>\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\ndf = pd.DataFrame([[1, 5, 2], [2, 4, 4], [3, 3, 1], [4, 2, 2], [5, 1, 4]], \n                 columns=['A', 'B', 'C'], index = [1, 2, 3, 4, 5])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df)\n</code>\n\n\ndef",
        "answer": "def tau1(x):\n    y = np.array(A['A']) #  keep one column fix and run it in the other two\n    tau, p_value = sp.stats.kendalltau(x, y)\n    return tau\n A['AB'] = pd.rolling_apply(A['B'], 3, lambda x: tau1(x))\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 45,
        "source_id": 45,
        "prompt": "Complete the Python function given the prompt below:Problem:\nWhat is the canonical way to check if a SciPy CSR matrix is empty (i.e. contains only zeroes)?\nI use nonzero():\ndef is_csr_matrix_only_zeroes(my_csr_matrix):\n    return(len(my_csr_matrix.nonzero()[0]) == 0)\nfrom scipy.sparse import csr_matrix\nprint(is_csr_matrix_only_zeroes(csr_matrix([[1,2,0],[0,0,3],[4,0,5]])))\nprint(is_csr_matrix_only_zeroes(csr_matrix([[0,0,0],[0,0,0],[0,0,0]])))\nprint(is_csr_matrix_only_zeroes(csr_matrix((2,3))))\nprint(is_csr_matrix_only_zeroes(csr_matrix([[0,0,0],[0,1,0],[0,0,0]])))\noutputs\nFalse\nTrue\nTrue\nFalse\nbut I wonder whether there exist more direct or efficient ways, i.e. just get True or False?\nA:\n<code>\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return(len(my_csr_matrix.nonzero()[0]) == 0)\nfrom scipy.sparse import csr_matrix\nprint(is_csr_matrix_only_zeroes(csr_matrix([[1,2,0],[0,0,3],[4,0,5]])))\nprint(is_csr_matrix_only_zeroes(csr_matrix([[0,0,0],[0,0,0],[0,0,0]])))\nprint(is_csr_matrix_only_zeroes(csr_matrix([[0,0,0],[0,0,0],[0,0,0]])))\noutputs\nFalse\nTrue\nTrue\nFalse\nbut I wonder whether there exist more direct or efficient ways, i.e. just get True or False?\nA:\n<code>\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 46,
        "source_id": 45,
        "prompt": "Complete the Python function given the prompt below:Problem:\nWhat is the canonical way to check if a SciPy lil matrix is empty (i.e. contains only zeroes)?\nI use nonzero():\ndef is_lil_matrix_only_zeroes(my_lil_matrix):\n    return(len(my_lil_matrix.nonzero()[0]) == 0)\nfrom scipy.sparse import csr_matrix\nprint(is_lil_matrix_only_zeroes(lil_matrix([[1,2,0],[0,0,3],[4,0,5]])))\nprint(is_lil_matrix_only_zeroes(lil_matrix([[0,0,0],[0,0,0],[0,0,0]])))\nprint(is_lil_matrix_only_zeroes(lil_matrix((2,3))))\nprint(is_lil_matrix_only_zeroes(lil_matrix([[0,0,0],[0,1,0],[0,0,0]])))\noutputs\nFalse\nTrue\nTrue\nFalse\nbut I wonder whether there exist more direct or efficient ways, i.e. just get True or False?\nA:\n<code>\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'lil')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return(len(my_lil_matrix.nonzero()[0]) == 0)\nfrom scipy.sparse import csr_matrix\nprint(is_lil_matrix_only_zeroes(lil_matrix([[1,2,0],[0,0,3],[4,0,5]])))\nprint(is_lil_matrix_only_zeroes(lil_matrix([[0,0,0],[0,0,0],[0,0,0]])))\nprint(is_lil_matrix_only_zeroes(lil_matrix((2,3))))\nprint(is_lil_matrix_only_zeroes(lil_matrix([[0,0,0],[0,1,0],[0,0,0]])))\noutputs\nFalse\nTrue\nTrue\nFalse\nbut I wonder whether there exist more direct or efficient ways, i.e. just get True or False?\nA:\n<code>\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'lil')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 47,
        "source_id": 47,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am looking for a way to convert a nXaXb numpy array into a block diagonal matrix. I have already came across scipy.linalg.block_diag, the down side of which (for my case) is it requires each blocks of the matrix to be given separately. However, this is challenging when n is very high, so to make things more clear lets say I have a \nimport numpy as np    \na = np.random.rand(3,2,2)\narray([[[ 0.33599705,  0.92803544],\n        [ 0.6087729,  0.8557143 ]],\n       [[ 0.81496749,  0.15694689],\n        [ 0.87476697,  0.67761456]],\n       [[ 0.11375185,  0.32927167],\n        [ 0.3456032,  0.48672131]]])\nwhat I want to achieve is something the same as \nfrom scipy.linalg import block_diag\nblock_diag(a[0], a[1],a[2])\narray([[ 0.33599705,  0.92803544,  0.       ,  0.       ,  0.       ,   0.        ],\n       [ 0.6087729,  0.8557143,  0.       ,  0.       ,  0.       ,   0.        ],\n       [ 0.       ,  0.       ,  0.81496749,  0.15694689,  0.       ,   0.        ],\n       [ 0.       ,  0.       ,  0.87476697,  0.67761456,  0.       ,   0.        ],\n       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.11375185,   0.32927167],\n       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.3456032,   0.48672131]])\nThis is just as an example in actual case a has hundreds of elements.\nA:\n<code>\nimport numpy as np\nfrom scipy.linalg import block_diag\nnp.random.seed(10)\na = np.random.rand(100,2,2)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def block_diag(a, b, c):\n    \"\"\"\n    a, b, c: numpy arrays\n    \"\"\"\n    return block_diag(a, b, c)"
    },
    {
        "library_id": 48,
        "source_id": 48,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have the following code to run Wilcoxon rank-sum test \nprint stats.ranksums(pre_course_scores, during_course_scores)\nRanksumsResult(statistic=8.1341352369246582, pvalue=4.1488919597127145e-16)\nHowever, I am interested in extracting the pvalue from the result. I could not find a tutorial about this. i.e.Given two ndarrays, pre_course_scores, during_course_scores, I want to know the pvalue of ranksum. Can someone help?\nA:\n<code>\nimport numpy as np\nfrom scipy import stats\nnp.random.seed(10)\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(p_value)\n</code>\n\n\ndef",
        "answer": "def p_value(pre_course_scores, during_course_scores):\n    return stats.ranksums(pre_course_scores, during_course_scores)[1]"
    },
    {
        "library_id": 49,
        "source_id": 48,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have the following code to run Wilcoxon rank-sum test \nprint stats.ranksums(pre_course_scores, during_course_scores)\nRanksumsResult(statistic=8.1341352369246582, pvalue=4.1488919597127145e-16)\nHowever, I am interested in extracting the pvalue from the result. I could not find a tutorial about this. i.e.Given two ndarrays, pre_course_scores, during_course_scores, I want to know the pvalue of ranksum. Can someone help?\nA:\n<code>\nimport numpy as np\nfrom scipy import stats\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\n\n\ndef",
        "answer": "def f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p_value\n</code>\nEND SOLUTION\n<code>\n    return p"
    },
    {
        "library_id": 50,
        "source_id": 50,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow to calculate kurtosis (the fourth standardized moment, according to Pearson\u2019s definition) without bias correction?\nI have tried scipy.stats.kurtosis, but it gives a different result. I followed the definition in mathworld.\nA:\n<code>\nimport numpy as np\na = np.array([   1.,    2.,    2.5,  400.,    6.,    0. ])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(kurtosis_result)\n</code>\n\n\ndef",
        "answer": "def kurtosis_result(a):\n    return np.sqrt(np.var(a))"
    },
    {
        "library_id": 51,
        "source_id": 51,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow to calculate kurtosis (according to Fisher\u2019s definition) without bias correction?\nA:\n<code>\nimport numpy as np\nimport scipy.stats\na = np.array([   1.,    2.,    2.5,  400.,    6.,    0. ])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(kurtosis_result)\n</code>\n\n\ndef",
        "answer": "def kurtosis(a):\n    return scipy.stats.kurtosis(a, bias=False)"
    },
    {
        "library_id": 52,
        "source_id": 52,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a table of measured values for a quantity that depends on two parameters. So say I have a function fuelConsumption(speed, temperature), for which data on a mesh are known.\nNow I want to interpolate the expected fuelConsumption for a lot of measured data points (speed, temperature) from a pandas.DataFrame (and return a vector with the values for each data point).\nI am currently using SciPy's interpolate.interp2d for cubic interpolation, but when passing the parameters as two vectors [s1,s2] and [t1,t2] (only two ordered values for simplicity) it will construct a mesh and return:\n[[f(s1,t1), f(s2,t1)], [f(s1,t2), f(s2,t2)]]\nThe result I am hoping to get is:\n[f(s1,t1), f(s2, t2)]\nHow can I interpolate to get the output I want?\nI want to use function interpolated on x, y, z to compute values on arrays s and t, and the result should be like mentioned above.\nA:\n<code>\nimport numpy as np\nimport scipy.interpolate\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 0, 50)\nx, y = np.ogrid[-1:1:10j,-2:0:10j]\nz = (x + y)*np.exp(-6.0 * (x * x + y * y))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def interpolate(s, t):\n    return scipy.interpolate.interp2d(s, t, z, kind='cubic')"
    },
    {
        "library_id": 53,
        "source_id": 52,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a table of measured values for a quantity that depends on two parameters. So say I have a function fuelConsumption(speed, temperature), for which data on a mesh are known.\nNow I want to interpolate the expected fuelConsumption for a lot of measured data points (speed, temperature) from a pandas.DataFrame (and return a vector with the values for each data point).\nI am currently using SciPy's interpolate.interp2d for cubic interpolation, but when passing the parameters as two vectors [s1,s2] and [t1,t2] (only two ordered values for simplicity) it will construct a mesh and return:\n[[f(s1,t1), f(s2,t1)], [f(s1,t2), f(s2,t2)]]\nThe result I am hoping to get is:\n[f(s1,t1), f(s2, t2)]\nHow can I interpolate to get the output I want?\nI want to use function interpolated on x, y, z to compute values on arrays s and t, and the result should be like mentioned above.\nA:\n<code>\nimport numpy as np\nimport scipy.interpolate\nexampls_s = np.linspace(-1, 1, 50)\nexample_t = np.linspace(-2, 0, 50)\ndef f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def f(s = example_s, t = example_t):\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    return scipy.interpolate.interp2d(s, t, z, kind='cubic')\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 54,
        "source_id": 54,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI think my questions has something in common with this question or others, but anyway, mine is not specifically about them.\nI would like, after having found the voronoi tessallination for certain points, be able to check where other given points sit within the tessellination. In particular:\nGiven say 50 extra-points, I want to be able to count how many of these extra points each voronoi cell contains.\nMy MWE\nfrom scipy.spatial import ConvexHull, Voronoi\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n#voronoi\nvor = Voronoi(points)\nNow I am given extra points\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# In this case we have that the first point is in the bottom left, \n# the successive three are in the bottom right and the last one\n# is in the top right cell.\nI was thinking to use the fact that you can get vor.regions or vor.vertices, however I really couldn't come up with anything..\nIs there parameter or a way to make this? The result I want is an np.array containing indices standing for regions occupied by different points, i.e., 1 for [1, 4]\u2019s region.\nA:\n<code>\nimport scipy.spatial\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def get_voronoi_cells(points):\n    \"\"\"\n    Returns a list of voronoi cells.\n    \"\"\"\n    # Create a voronoi cell\n    cell = ConvexHull(points)\n    # Get the indices of the points in the cell\n    indices = cell.vertices\n    # Return the list of indices\n    return indices"
    },
    {
        "library_id": 55,
        "source_id": 54,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI think my questions has something in common with this question or others, but anyway, mine is not specifically about them.\nI would like, after having found the voronoi tessallination for certain points, be able to check where other given points sit within the tessellination. In particular:\nGiven say 50 extra-points, I want to be able to count how many of these extra points each voronoi cell contains.\nMy MWE\nfrom scipy.spatial import ConvexHull, Voronoi\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n#voronoi\nvor = Voronoi(points)\nNow I am given extra points\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# In this case we have that the first point is in the bottom left, \n# the successive three are in the bottom right and the last one\n# is in the top right cell.\nI was thinking to use the fact that you can get vor.regions or vor.vertices, however I really couldn't come up with anything..\nIs there parameter or a way to make this? The result I want is an np.array containing indices standing for regions occupied by different points, and that should be defined by Voronoi cell.\nA:\n<code>\nimport scipy.spatial\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def voronoi_tess(points):\n    # Create a voronoi object\n    vor = Voronoi(points)\n    # Get the vertices of the voronoi object\n    vertices = vor.vertices\n    # Get the indices of the vertices\n    indices = vor.regions\n    # Get the number of vertices\n    n = len(vertices)\n    # Create an empty array to store the indices\n    indices_array = np.empty(n, dtype=np.int32)\n    # Fill the array with the indices\n    indices_array[:] = indices\n    # Return the array\n    return indices_array"
    },
    {
        "library_id": 56,
        "source_id": 56,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a list of numpy vectors of the format:\n    [array([[-0.36314615,  0.80562619, -0.82777381,...,  2.00876354,2.08571887, -1.24526026]]), \n     array([[ 0.9766923, -0.05725135, -0.38505339,...,  0.12187988,-0.83129255,  0.32003683]]),\n     array([[-0.59539878,  2.27166874,  0.39192573,..., -0.73741573,1.49082653,  1.42466276]])]\nhere, only 3 vectors in the list are shown. I have 100s..\nThe maximum number of elements in one vector is around 10 million\nAll the arrays in the list have unequal number of elements but the maximum number of elements is fixed.\nIs it possible to create a sparse matrix using these vectors in python such that I have padded zeros to the end of elements for the vectors which are smaller than the maximum size?\nA:\n<code>\nimport numpy as np\nimport scipy.sparse as sparse\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def create_sparse_matrix(vectors):\n    \"\"\"\n    Creates a sparse matrix from a list of vectors.\n    \"\"\"\n    sparse_matrix = sparse.csr_matrix(vectors)\n    return sparse_matrix"
    },
    {
        "library_id": 57,
        "source_id": 57,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a binary array, say, a = np.random.binomial(n=1, p=1/2, size=(9, 9)). I perform median filtering on it using a 3 x 3 kernel on it, like say, b = nd.median_filter(a, 3). I would expect that this should perform median filter based on the pixel and its eight neighbours. However, I am not sure about the placement of the kernel. The documentation says,\norigin : scalar, optional.\nThe origin parameter controls the placement of the filter. Default 0.0.\nNow, I want to shift this filter one cell to the right.How can I achieve it?\nThanks.\nA:\n<code>\nimport numpy as np\nimport scipy.ndimage\na= np.zeros((5, 5))\na[1:4, 1:4] = np.arange(3*3).reshape((3, 3))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\n\n\ndef",
        "answer": "def median_filter(a, origin=0.0):\n    \"\"\"\n    Performs median filtering on a 2D array."
    },
    {
        "library_id": 58,
        "source_id": 58,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a sparse matrix in csr format (which makes sense for my purposes, as it has lots of rows but relatively few columns, ~8million x 90).\nMy question is, what's the most efficient way to access a particular value from the matrix given a row,column tuple? I can quickly get a row using matrix.getrow(row), but this also returns 1-row sparse matrix, and accessing the value at a particular column seems clunky. \nThe only reliable method I've found to get a particular matrix value, given the row and column, is:\ngetting the row vector, converting to dense array, and fetching the element on column.\nBut this seems overly verbose and complicated. and I don't want to change it to dense matrix to keep the efficiency.\nIs there a simpler/faster method I'm missing?\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = 2\ncolumn = 3\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def get_row(row):\n    return row[0]"
    },
    {
        "library_id": 59,
        "source_id": 58,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a sparse matrix in csr format (which makes sense for my purposes, as it has lots of rows but relatively few columns, ~8million x 90).\nMy question is, what's the most efficient way to access particular values from the matrix given lists of row,column indices? I can quickly get a row using matrix.getrow(row), but this also returns 1-row sparse matrix, and accessing the value at a particular column seems clunky. The only reliable method I've found to get a particular matrix value, given the row and column, is:\ngetting the row vector, converting to dense array, and fetching the element on column.\nBut this seems overly verbose and complicated. and I don't want to change it to dense matrix to keep the efficiency.\nfor example, I want to fetch elements at (2, 3) and (1, 0), so row = [2, 1], and column = [3, 0].\nThe result should be a list or 1-d array like: [matirx[2, 3], matrix[1, 0]]\nIs there a simpler/faster method I'm missing?\nA:\n<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = [2, 1]\ncolumn = [3, 0]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def getrow(row, column):\n    return row[column]"
    },
    {
        "library_id": 60,
        "source_id": 60,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have an array which I want to interpolate over the 1st axes. At the moment I am doing it like this example:\nimport numpy as np\nfrom scipy.interpolate import interp1d\narray = np.random.randint(0, 9, size=(100, 100, 100))\nnew_array = np.zeros((1000, 100, 100))\nx = np.arange(0, 100, 1)\nx_new = np.arange(0, 100, 0.1)\nfor i in x:\n    for j in x:\n        f = interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(xnew)\nThe data I use represents 10 years of 5-day averaged values for each latitude and longitude in a domain. I want to create an array of daily values.\nI have also tried using splines. I don't really know how they work but it was not much faster.\nIs there a way to do this without using for loops? The result I want is an np.array of transformed x_new values using interpolated function.\nThank you in advance for any suggestions.\nA:\n<code>\nimport numpy as np\nimport scipy.interpolate\narray = np.random.randint(0, 9, size=(10, 10, 10))\nx = np.linspace(0, 10, 10)\nx_new = np.linspace(0, 10, 100)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(new_array)\n</code>\n\n\ndef",
        "answer": "def interpolate(x, y):\n    return scipy.interpolate.interp1d(x, y)(x_new)"
    },
    {
        "library_id": 61,
        "source_id": 61,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to integrate X (X ~ N(u, o2)) to calculate the probability up to position `x`.\nHowever I'm running into an error of:\nTraceback (most recent call last):\n  File \"<ipython console>\", line 1, in <module>\n  File \"siestats.py\", line 349, in NormalDistro\n    P_inner = scipy.integrate(NDfx,-dev,dev)\nTypeError:'module' object is not callable\nMy code runs this:\n# Definition of the mathematical function:\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\n# This Function normailizes x, u, and o2 (position of interest, mean and st dev) \n# and then calculates the probability up to position 'x'\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nA:\n<code>\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\nx = 2.5\nu = 1\no2 = 3\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(prob)\n</code>\n\n\ndef",
        "answer": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nA:\n<code>\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\nx = 2.5\nu = 1\no2 = 3\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(prob)\n</code>"
    },
    {
        "library_id": 62,
        "source_id": 61,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to integrate X (X ~ N(u, o2)) to calculate the probability up to position `x`.\nHowever I'm running into an error of:\nTraceback (most recent call last):\n  File \"<ipython console>\", line 1, in <module>\n  File \"siestats.py\", line 349, in NormalDistro\n    P_inner = scipy.integrate(NDfx,-dev,dev)\nTypeError:'module' object is not callable\nMy code runs this:\n# Definition of the mathematical function:\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\n# This Function normailizes x, u, and o2 (position of interest, mean and st dev) \n# and then calculates the probability up to position 'x'\ndef NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nA:\n<code>\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return prob\n</code>\n\n\ndef",
        "answer": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nA:\n<code>\nimport scipy.integrate\nimport math\nimport numpy as np\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\ndef f(x = 2.5, u = 1, o2 = 3):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return prob\n</code>"
    },
    {
        "library_id": 63,
        "source_id": 63,
        "prompt": "Complete the Python function given the prompt below:Problem:\nUsing scipy, is there an easy way to emulate the behaviour of MATLAB's dctmtx function which returns a NxN (ortho-mode normed) DCT matrix for some given N? There's scipy.fftpack.dctn but that only applies the DCT. Do I have to implement this from scratch if I don't want use another dependency besides scipy?\nA:\n<code>\nimport numpy as np\nimport scipy.fft as sf\nN = 8\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def dctmtx(x):\n    N = len(x)\n    result = np.zeros(N, dtype=np.complex64)\n    for i in range(N):\n        result[i] = sf.dctn(x[i], type=2)\n    return result"
    },
    {
        "library_id": 64,
        "source_id": 64,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHaving difficulty generating a tridiagonal matrix from numpy arrays. I managed to replicate the results given here, but I'm not able to apply these techniques to my problem. I may also be misunderstanding the application of scipy.sparse.diag.\nFor context, I'm working on a problem which requires the generation of a tridiagonal matrix to solve an ordinary differential equation numerically using finite differences.\nfrom scipy.sparse import diags\nimport numpy as np\nv1 = [3*i**2 +(i/2) for i in range(1, 6)]\nv2 = [-(6*i**2 - 1) for i in range(1, 6)]\nv3 = [3*i**2 -(i/2) for i in range(1, 6)]\nmatrix = np.array([v1, v2, v3])\nmatrix is equal to.\narray([[3.5,   13.,   28.5,   50.,   77.5],\n       [-5.,  -23.,  -53.,  -95., -149. ],\n       [2.5,   11.,   25.5,   46.,   72.5]])\nAfter working through the Scipy documentation and the examples in the link above, I was expecting the following code to yield Tridiagonal_1, but instead get Tridiagonal_2.\ndiags(matrix, [-1,0,1], (5, 5)).toarray() \nexpected Tridiagonal_1:\narray([[  -5.,    2.5,     0.,    0.,     0. ],\n       [  13.,   -23.,    11.,    0.,     0. ],\n       [   0.,    28.5.,  -53.,   25.5,     0. ],\n       [   0.,    0.,     50,   -95.,     46. ],\n       [   0.,    0.,      0.,   77.5., -149. ]])\nCode yielded Tridiagonal_2:\narray([[  -5.,    2.5,    0.,    0.,    0. ],\n       [   3.5,  -23.,   11.,    0.,    0. ],\n       [   0.,   13.,  -53.,   25.5,    0. ],\n       [   0.,    0.,   28.5,  -95.,   46. ],\n       [   0.,    0.,    0.,   50., -149. ]])\nI was expecting offset = [-1,0,1] to shift the diagonal entries to the left, but the first offset is shifting the first diag to the next row. Is this correct or is there an error in my code causing this behaviour?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nmatrix = np.array([[3.5,   13.,   28.5,   50.,   77.5],\n                   [-5.,  -23.,  -53.,  -95., -149. ],\n                   [2.5,   11.,   25.5,   46.,   72.5]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def tridiagonal_1(matrix):\n    return np.diag(matrix, -1) + np.diag(matrix, 0) + np.diag(matrix, 1)"
    },
    {
        "library_id": 65,
        "source_id": 65,
        "prompt": "Complete the Python function given the prompt below:Problem:\nGive the N and P, I want to get a 2D binomial distribution probability matrix M,\nfor i in range(N+1):\n   for j in range(i+1):\n      M[i,j] = choose(i, j) * p**j * (1-p)**(i-j)\nother value = 0\nI want to know is there any fast way to get this matrix, instead of the for loop. the N may be bigger than 100,000\nA:\n<code>\nimport numpy as np\nimport scipy.stats\nN = 3\np = 0.5\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def binomial(N, p):\n    result = np.zeros((N+1, N+1))\n    for i in range(N+1):\n        for j in range(i+1):\n            result[i,j] = choose(i, j) * p**j * (1-p)**(i-j)\n    return result"
    },
    {
        "library_id": 66,
        "source_id": 66,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have the following data frame:\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\ndf\nIt looks like this\n                     sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1       20        0       11\n1415805_at Clps           17        0       55\n1415884_at Cela3b         47        0      100\nWhat I want to do is too perform row-zscore calculation using SCIPY. At the end of the day. the result will look like:\n                               sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1      1.18195176, -1.26346568,  0.08151391\n1415805_at Clps         -0.30444376, -1.04380717,  1.34825093\n1415884_at Cela3b        -0.04896043, -1.19953047,  1.2484909\nA:\n<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def zscore(df):\n    \"\"\"\n    Calculate the z-score of the dataframe.\n    \"\"\"\n    df = df.copy()\n    df['sample1'] = df['sample1'].astype(float)\n    df['sample2'] = df['sample2'].astype(float)\n    df['sample3'] = df['sample3'].astype(float)\n    df['sample1'] = (df['sample1'] - df['sample1'].mean()) / df['sample1'].std()\n    df['sample2'] = (df['sample2'] - df['sample2'].mean()) / df['sample2'].std()\n    df['sample3'] = (df['sample3'] - df['sample3'].mean()) / df['sample3'].std()\n    return df"
    },
    {
        "library_id": 67,
        "source_id": 66,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have the following data frame:\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\ndf\nIt looks like this\n                     sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1       20        0       11\n1415805_at Clps           17        0       55\n1415884_at Cela3b         47        0      100\nWhat I want to do is too perform column-zscore calculation using SCIPY. At the end of the day. the result will look like:\n                               sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1             x.xxxxxxxx,    x.xxxxxxxx,  x.xxxxxxxx\n1415805_at Clps                 x.xxxxxxxx,    x.xxxxxxxx,  x.xxxxxxxx\n1415884_at Cela3b               x.xxxxxxxx,    x.xxxxxxxx,  x.xxxxxxxx\nA:\n<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def zscore(df,col):\n    return (df[col]-df[col].mean())/df[col].std()"
    },
    {
        "library_id": 68,
        "source_id": 66,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have the following data frame:\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\ndf\nIt looks like this\n                     sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1       20        0       11\n1415805_at Clps           17        0       55\n1415884_at Cela3b         47        0      100\nWhat I want to do is too perform row-zscore calculation using SCIPY. AND I want to show data and zscore together in a single dataframe. At the end of the day. the result will look like:\n                               sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1   data     20          0            11\n                    zscore      1.18195176 -1.26346568  0.08151391\n1415805_at Clps          data     17          0            55\n                    zscore   -0.30444376 -1.04380717  1.34825093\n1415884_at Cela3b      data     47          0            100\n                    zscore   -0.04896043 -1.19953047  1.2484909\nA:\n<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def zscore(df,col):\n    return df[col].apply(lambda x: (x - x.mean()) / (x.std()))"
    },
    {
        "library_id": 69,
        "source_id": 66,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have the following data frame:\nimport pandas as pd\nimport io\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\ndf\nIt looks like this\n                     sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1       20        0       11\n1415805_at Clps           17        0       55\n1415884_at Cela3b         47        0      100\nWhat I want to do is too perform column-zscore calculation using SCIPY. AND I want to show data and zscore together in a single dataframe. For each element, I want to only keep 3 decimals places. At the end of the day. the result will look like:\n                               sample1  sample2  sample3\nprobegenes\n1415777_at Pnliprp1   data     20.000    0.000    11.000\n                    zscore       -0.593    NaN    -1.220\n1415805_at Clps          data     17.000    0.000    55.000\n                    zscore     -0.815    NaN    -0.009\n1415884_at Cela3b      data     47.000    0.000    100.000\n                    zscore     1.408     NaN     1.229\nA:\n<code>\nimport pandas as pd\nimport io\nimport numpy as np\nfrom scipy import stats\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def zscore(data):\n    return (data - data.mean()) / data.std()"
    },
    {
        "library_id": 70,
        "source_id": 70,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm searching for examples of using scipy.optimize.line_search. I do not really understand how this function works with multivariable functions. I wrote a simple example\nimport scipy as sp\nimport scipy.optimize\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nsp.optimize.line_search(test_func,test_grad,[1.8,1.7],[-1.0,-1.0])\nAnd I've got\nFile \"D:\\Anaconda2\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 259, in phi\nreturn f(xk + alpha * pk, *args)\nTypeError: can't multiply sequence by non-int of type 'float'\nThe result should be the alpha value of line_search\nA:\n<code>\nimport scipy\nimport scipy.optimize\nimport numpy as np\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def test_func(x):\n    return (x[0])**2+(x[1])**2\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\n<code>\nalpha = 0.01\n<code>\nresult = scipy.optimize.linesearch(test_func,test_grad,starting_point,direction,alpha)\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION"
    },
    {
        "library_id": 71,
        "source_id": 71,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to create a 2-dimensional array in Scipy/Numpy where each value represents the euclidean distance from the center.\nI'm very new to Scipy, and would like to know if there's a more elegant, idiomatic way of doing the same thing. I found the scipy.spatial.distance.cdist function, which seems promising, but I'm at a loss regarding how to fit it into this problem.\ndef get_distance_2(y, x):\n    mid =...  # needs to be a array of the shape (rows, cols, 2)?\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)\nJust to clarify, what I'm looking for is something like this (for a 6 x 6 array). That is, to compute (Euclidean) distances from center point to every point in the image.\n[[ 3.53553391  2.91547595  2.54950976  2.54950976  2.91547595  3.53553391]\n [ 2.91547595  2.12132034  1.58113883  1.58113883  2.12132034  2.91547595]\n [ 2.54950976  1.58113883  0.70710678  0.70710678  1.58113883  2.54950976]\n [ 2.54950976  1.58113883  0.70710678  0.70710678  1.58113883  2.54950976]\n [ 2.91547595  2.12132034  1.58113883  1.58113883  2.12132034  2.91547595]\n [ 3.53553391  2.91547595  2.54950976  2.54950976  2.91547595  3.53553391]]\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def get_distance_2(y, x):\n    mid =...  # needs to be a array of the shape (rows, cols, 2)?\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)\nEND SOLUTION"
    },
    {
        "library_id": 72,
        "source_id": 71,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to create a 2-dimensional array in Scipy/Numpy where each value represents the Manhattan distance from the center. It's supposed to have the same shape as the first two dimensions of a 3-dimensional array (an image, created via scipy.misc.fromimage).\nI'm very new to Scipy, and would like to know if there's a more elegant, idiomatic way of doing the same thing. I found the scipy.spatial.distance.cdist function, which seems promising, but I'm at a loss regarding how to fit it into this problem.\ndef get_distance_2(y, x):\n    mid =...  # needs to be a array of the shape (rows, cols, 2)?\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)\nJust to clarify, what I'm looking for is something like this (for a 6 x 6 array). That is, to compute Manhattan distances from center point to every point in the image.\n[[5., 4., 3., 3., 4., 5.],\n       [4., 3., 2., 2., 3., 4.],\n       [3., 2., 1., 1., 2., 3.],\n       [3., 2., 1., 1., 2., 3.],\n       [4., 3., 2., 2., 3., 4.],\n       [5., 4., 3., 3., 4., 5.]]\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\nshape = (6, 6)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def get_distance_2(y, x):\n    mid =...  # needs to be a array of the shape (rows, cols, 2)?\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)"
    },
    {
        "library_id": 73,
        "source_id": 71,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm trying to create a 2-dimensional array in Scipy/Numpy where each value represents the euclidean distance from the center. It's supposed to have the same shape as the first two dimensions of a 3-dimensional array (an image, created via scipy.misc.fromimage).\nI'm very new to Scipy, and would like to know if there's a more elegant, idiomatic way of doing the same thing. I found the scipy.spatial.distance.cdist function, which seems promising, but I'm at a loss regarding how to fit it into this problem.\ndef get_distance_2(y, x):\n    mid =...  # needs to be a array of the shape (rows, cols, 2)?\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)\nJust to clarify, what I'm looking for is something like this (for a 6 x 6 array). That is, to compute (Euclidean) distances from center point to every point in the image.\n[[ 3.53553391  2.91547595  2.54950976  2.54950976  2.91547595  3.53553391]\n [ 2.91547595  2.12132034  1.58113883  1.58113883  2.12132034  2.91547595]\n [ 2.54950976  1.58113883  0.70710678  0.70710678  1.58113883  2.54950976]\n [ 2.54950976  1.58113883  0.70710678  0.70710678  1.58113883  2.54950976]\n [ 2.91547595  2.12132034  1.58113883  1.58113883  2.12132034  2.91547595]\n [ 3.53553391  2.91547595  2.54950976  2.54950976  2.91547595  3.53553391]]\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def get_distance_2(y, x):\n    mid =...  # needs to be a array of the shape (rows, cols, 2)?\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)\nEND SOLUTION"
    },
    {
        "library_id": 74,
        "source_id": 74,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI would like to resample a numpy array as suggested here Resampling a numpy array representing an image however this resampling will do so by a factor i.e.\nx = np.arange(9).reshape(3,3)\nprint scipy.ndimage.zoom(x, 2, order=1)\nWill create a shape of (6,6) but how can I resample an array to its best approximation within a (4,6),(6,8) or (6,10) shape for instance?\nA:\n<code>\nimport numpy as np\nimport scipy.ndimage\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def resample(x, shape):\n    \"\"\"\n    Resamples an array to a given shape.\n    \"\"\"\n    return scipy.ndimage.zoom(x, shape, order=1)"
    },
    {
        "library_id": 75,
        "source_id": 75,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am having a problem with minimization procedure. Actually, I could not create a correct objective function for my problem.\nProblem definition\n\u2022    My function: yn = a_11*x1**2 + a_12*x2**2 +... + a_m*xn**2,where xn- unknowns, a_m - coefficients. n = 1..N, m = 1..M\n\u2022    In my case, N=5 for x1,..,x5 and M=3 for y1, y2, y3.\nI need to find the optimum: x1, x2,...,x5 so that it can satisfy the y\nMy question:\n\u2022    How to solve the question using scipy.optimize?\nMy code:   (tried in lmfit, but return errors. Therefore I would ask for scipy solution)\nimport numpy as np\nfrom lmfit import Parameters, minimize\ndef func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model) **2\ndef main():\n    # simple one: a(M,N) = a(3,5)\n    a = np.array([ [ 0, 0, 1, 1, 1 ],\n                   [ 1, 0, 1, 0, 1 ],\n                   [ 0, 1, 0, 1, 0 ] ])\n    # true values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n    # data without noise\n    y = func(x_true,a)\n    #************************************\n    # Apriori x0\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print out\nif __name__ == '__main__':\nmain()\nResult should be optimal x array.\nA:\n<code>\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(out)\n</code>\n\n\ndef",
        "answer": "def func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model) **2\ndef main():\n    # simple one: a(M,N) = a(3,5)\n    a = np.array([ [ 0, 0, 1, 1, 1 ],\n                   [ 1, 0, 1, 0, 1 ],\n                   [ 0, 1, 0, 1, 0 ] ])\n    # true values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n    # data without noise\n    y = func(x_true,a)\n    #************************************\n    # Apriori x0\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print out\nif __name__ == '__main__':\nmain()\nResult should be optimal x array.\nA:\n<code>\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(out)\n</code>"
    },
    {
        "library_id": 76,
        "source_id": 75,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am having a problem with minimization procedure. Actually, I could not create a correct objective function for my problem.\nProblem definition\n\u2022    My function: yn = a_11*x1**2 + a_12*x2**2 +... + a_m*xn**2,where xn- unknowns, a_m - coefficients. n = 1..N, m = 1..M\n\u2022    In my case, N=5 for x1,..,x5 and M=3 for y1, y2, y3.\nI need to find the optimum: x1, x2,...,x5 so that it can satisfy the y\nMy question:\n\u2022    How to solve the question using scipy.optimize?\nMy code:   (tried in lmfit, but return errors. Therefore I would ask for scipy solution)\nimport numpy as np\nfrom lmfit import Parameters, minimize\ndef func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model)**2\ndef main():\n    # simple one: a(M,N) = a(3,5)\n    a = np.array([ [ 0, 0, 1, 1, 1 ],\n                   [ 1, 0, 1, 0, 1 ],\n                   [ 0, 1, 0, 1, 0 ] ])\n    # true values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n    # data without noise\n    y = func(x_true,a)\n    #************************************\n    # Apriori x0\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print out\nif __name__ == '__main__':\nmain()\nResult should be optimal x array. The method I hope to use is L-BFGS-B, with added lower bounds on x.\nA:\n<code>\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(out)\n</code>\n\n\ndef",
        "answer": "def func(x,a):\n    return np.dot(a, x**2)\ndef residual(pars, a, y):\n    vals = pars.valuesdict()\n    x = vals['x']\n    model = func(x,a)\n    return (y - model)**2\ndef main():\n    # simple one: a(M,N) = a(3,5)\n    a = np.array([ [ 0, 0, 1, 1, 1 ],\n                   [ 1, 0, 1, 0, 1 ],\n                   [ 0, 1, 0, 1, 0 ] ])\n    # true values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n    # data without noise\n    y = func(x_true,a)\n    #************************************\n    # Apriori x0\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = minimize(residual, fit_params, args=(a, y))\n    print out\nif __name__ == '__main__':\nmain()\nResult should be optimal x array. The method I hope to use is L-BFGS-B, with added lower bounds on x.\nA:\n<code>\nimport scipy.optimize\nimport numpy as np\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(out)\n</code>"
    },
    {
        "library_id": 77,
        "source_id": 77,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI\u2019m trying to solve a simple ODE to visualise the temporal response, which works well for constant input conditions using the new solve_ivp integration API in SciPy. For example:\ndef dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nHowever, I wonder is it possible to plot the response to a time-varying input? For instance, rather than having y0 fixed at N0, can I find the response to a simple sinusoid? Specifically, I want to change dy/dt = -100*y + sin(t) to let it become time-variant. The result I want is values of solution at time points.\nIs there a compatible way to pass time-varying input conditions into the API?\nA:\n<code>\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nresult = sol.y\nprint(result)\n</code>\n\n\ndef",
        "answer": "def dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])"
    },
    {
        "library_id": 78,
        "source_id": 77,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI\u2019m trying to solve a simple ODE to visualise the temporal response, which works well for constant input conditions using the new solve_ivp integration API in SciPy. For example:\ndef dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=[0, 100e-3], y0=[N0,])\nHowever, I wonder is it possible to plot the response to a time-varying input? For instance, rather than having y0 fixed at N0, can I find the response to a simple sinusoid? Specifically, I want to add `t-sin(t) if 0 < t < 2pi else 2pi` to original y. The result I want is values of solution at time points.\nIs there a compatible way to pass time-varying input conditions into the API?\nA:\n<code>\nimport scipy.integrate\nimport numpy as np\nN0 = 1\ntime_span = [0, 10]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nresult = sol.y\nprint(result)\n</code>\n\n\ndef",
        "answer": "def dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=[0, 100e-3], y0=[N0,])"
    },
    {
        "library_id": 79,
        "source_id": 77,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI\u2019m trying to solve a simple ODE to visualise the temporal response, which works well for constant input conditions using the new solve_ivp integration API in SciPy. For example:\ndef dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nHowever, I wonder is it possible to plot the response to a time-varying input? For instance, rather than having y0 fixed at N0, can I find the response to a simple sinusoid? Specifically, I want to add `-cos(t)` to original y. The result I want is values of solution at time points.\nIs there a compatible way to pass time-varying input conditions into the API?\nA:\n<code>\nimport scipy.integrate\nimport numpy as np\nN0 = 10\ntime_span = [-0.1, 0.1]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nresult = sol.y\nprint(result)\n</code>\n\n\ndef",
        "answer": "def dN1_dt_simple(t, N1):\n    return -100 * N1\nsol = solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])"
    },
    {
        "library_id": 80,
        "source_id": 80,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI'm using scipy.optimize.minimize to solve a complex reservoir optimization model (SQSLP and COBYLA as the problem is constrained by both bounds and constraint equations). There is one decision variable per day (storage), and releases from the reservoir are calculated as a function of change in storage, within the objective function. Penalties based on releases and storage penalties are then applied with the goal of minimizing penalties (the objective function is a summation of all penalties). I've added some constraints within this model to limit the change in storage to the physical system limits which is the difference between decision variable x(t+1) and x(t), and also depends on inflows at that time step I(t). These constraints are added to the list of constraint dictionaries using a for loop. Constraints added outside of this for loop function as they should. However the constraints involving time that are initiated within the for loop, do not.\nObviously the problem is complex so I've recreated a simpler version to illustrate the problem. This problem has four decision variables and seeks to minimize the objective function (which I've called function) with constraints of steady state (I = inflow must equal x = outflow) and non negativity (ie. outflows x cannot be negative):\n    import numpy as np\n    from scipy.optimize import minimize\n    def function(x):\n        return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\n    I=np.array((20,50,50,80))\n    x0=I\n    cons=[]\n    steadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\n    cons.append(steadystate)\n    for t in range (4):\n        def const(x):    \n            y=x[t]\n            return y\n        cons.append({'type':'ineq', 'fun': const})\n    out=minimize(function, x0, method=\"SLSQP\", constraints=cons)\n    x=out[\"x\"]\nThe constraints initiated in the for loop are non-negativity constraints but the optimization gives negative values for the decision variables. It does adhere to the steadystate constraint, however.\nAny ideas where I'm going wrong? I've seen constraints initiated similarly in other applications so I can't figure it out but assume it's something simple. I have hundreds of constraints to initiate in my full-scale version of this code so writing them out as in the second example will not be ideal.\nA:\n<code>\nimport numpy as np\nfrom scipy.optimize import minimize\ndef function(x):\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\nI=np.array((20,50,50,80))\nx0=I\ncons=[]\nsteadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\ncons.append(steadystate)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\n\n\ndef",
        "answer": "def function(x):\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\nI=np.array((20,50,50,80))\nx0=I\ncons=[]\nsteadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\ncons.append(steadystate)\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</code>\nEND SOLUTION\n<code>\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n</"
    },
    {
        "library_id": 81,
        "source_id": 81,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have problems using scipy.sparse.csr_matrix:\nfor instance:\na = csr_matrix([[1,2,3],[4,5,6]])\nb = csr_matrix([[7,8,9],[10,11,12]])\nhow to merge them into\n[[1,2,3],[4,5,6],[7,8,9],[10,11,12]]\nI know a way is to transfer them into numpy array first:\ncsr_matrix(numpy.vstack((a.toarray(),b.toarray())))\nbut it won't work when the matrix is huge and sparse, because the memory would run out.\nso are there any way to merge them together in csr_matrix?\nany answers are appreciated!\nA:\n<code>\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def csr_matrix(a,b):\n    return sparse.csr_matrix((a,b))"
    },
    {
        "library_id": 82,
        "source_id": 81,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have problems using scipy.sparse.csr_matrix:\nfor instance:\na = csr_matrix([[1,2,3],[4,5,6]])\nb = csr_matrix([[7,8,9],[10,11,12]])\nhow to merge them into\n[[1,2,3,7,8,9],[4,5,6,10,11,12]]\nI know a way is to transfer them into numpy array first:\ncsr_matrix(numpy.hstack((a.toarray(),b.toarray())))\nbut it won't work when the matrix is huge and sparse, because the memory would run out.\nso are there any way to merge them together in csr_matrix?\nany answers are appreciated!\nA:\n<code>\nfrom scipy import sparse\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def csr_matrix(a,b):\n    return sparse.csr_matrix((a,b))"
    },
    {
        "library_id": 83,
        "source_id": 83,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI would like to write a program that solves the definite integral below in a loop which considers a different value of the constant c per iteration.\nI would then like each solution to the integral to be outputted into a new array.\nHow do I best write this program in python?\n\u222b2cxdx with limits between 0 and 1.\nfrom scipy import integrate\nintegrate.quad\nIs acceptable here. My major struggle is structuring the program.\nHere is an old attempt (that failed)\n# import c\nfn = 'cooltemp.dat'\nc = loadtxt(fn,unpack=True,usecols=[1])\nI=[]\nfor n in range(len(c)):\n    # equation\n    eqn = 2*x*c[n]\n    # integrate \n    result,error = integrate.quad(lambda x: eqn,0,1)\n    I.append(result)\nI = array(I)\nA:\n<code>\nimport scipy.integrate\nc = 5\nlow = 0\nhigh = 1\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def f(x):\n    return 2*x*c[0]"
    },
    {
        "library_id": 84,
        "source_id": 83,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI would like to write a program that solves the definite integral below in a loop which considers a different value of the constant c per iteration.\nI would then like each solution to the integral to be outputted into a new array.\nHow do I best write this program in python?\n\u222b2cxdx with limits between 0 and 1.\nfrom scipy import integrate\nintegrate.quad\nIs acceptable here. My major struggle is structuring the program.\nHere is an old attempt (that failed)\n# import c\nfn = 'cooltemp.dat'\nc = loadtxt(fn,unpack=True,usecols=[1])\nI=[]\nfor n in range(len(c)):\n    # equation\n    eqn = 2*x*c[n]\n    # integrate \n    result,error = integrate.quad(lambda x: eqn,0,1)\n    I.append(result)\nI = array(I)\nA:\n<code>\nimport scipy.integrate\ndef f(c=5, low=0, high=1):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\n\n\ndef",
        "answer": "def f(c=5, low=0, high=1):\n    return 2*x*c[n]"
    },
    {
        "library_id": 85,
        "source_id": 85,
        "prompt": "Complete the Python function given the prompt below:Problem:\nFirst off, I'm no mathmatician. I admit that. Yet I still need to understand how ScyPy's sparse matrices work arithmetically in order to switch from a dense NumPy matrix to a SciPy sparse matrix in an application I have to work on. The issue is memory usage. A large dense matrix will consume tons of memory.\nThe formula portion at issue is where a matrix is added to a scalar.\nA = V + x\nWhere V is a square sparse matrix (its large, say 60,000 x 60,000). x is a float.\nWhat I want is that x will only be added to non-zero values in V.\nWith a SciPy, not all sparse matrices support the same features, like scalar addition. dok_matrix (Dictionary of Keys) supports scalar addition, but it looks like (in practice) that it's allocating each matrix entry, effectively rendering my sparse dok_matrix as a dense matrix with more overhead. (not good)\nThe other matrix types (CSR, CSC, LIL) don't support scalar addition.\nI could try constructing a full matrix with the scalar value x, then adding that to V. I would have no problems with matrix types as they all seem to support matrix addition. However I would have to eat up a lot of memory to construct x as a matrix, and the result of the addition could end up being fully populated matrix as well.\nThere must be an alternative way to do this that doesn't require allocating 100% of a sparse matrix. I\u2019d like to solve the problem on dok matrix first.\nI'm will to accept that large amounts of memory are needed, but I thought I would seek some advice first. Thanks.\nA:\n<code>\nimport numpy as np\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\nx = 99\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(V)\n</code>\n\n\ndef",
        "answer": "def add_x(x):\n    return x + x"
    },
    {
        "library_id": 86,
        "source_id": 85,
        "prompt": "Complete the Python function given the prompt below:Problem:\nFirst off, I'm no mathmatician. I admit that. Yet I still need to understand how ScyPy's sparse matrices work arithmetically in order to switch from a dense NumPy matrix to a SciPy sparse matrix in an application I have to work on. The issue is memory usage. A large dense matrix will consume tons of memory.\nThe formula portion at issue is where a matrix is added to a scalar.\nA = V + x\nWhere V is a square sparse matrix (its large, say 60,000 x 60,000). x is a float.\nWhat I want is that x will only be added to non-zero values in V.\nWith a SciPy, not all sparse matrices support the same features, like scalar addition. dok_matrix (Dictionary of Keys) supports scalar addition, but it looks like (in practice) that it's allocating each matrix entry, effectively rendering my sparse dok_matrix as a dense matrix with more overhead. (not good)\nThe other matrix types (CSR, CSC, LIL) don't support scalar addition.\nI could try constructing a full matrix with the scalar value x, then adding that to V. I would have no problems with matrix types as they all seem to support matrix addition. However I would have to eat up a lot of memory to construct x as a matrix, and the result of the addition could end up being fully populated matrix as well.\nThere must be an alternative way to do this that doesn't require allocating 100% of a sparse matrix. I\u2019d like to solve the problem on coo matrix first.\nI'm will to accept that large amounts of memory are needed, but I thought I would seek some advice first. Thanks.\nA:\n<code>\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(V)\n</code>\n\n\ndef",
        "answer": "def add_scalar(V, x):\n    V = V + x\n    return V"
    },
    {
        "library_id": 87,
        "source_id": 85,
        "prompt": "Complete the Python function given the prompt below:Problem:\nFirst off, I'm no mathmatician. I admit that. Yet I still need to understand how ScyPy's sparse matrices work arithmetically in order to switch from a dense NumPy matrix to a SciPy sparse matrix in an application I have to work on. The issue is memory usage. A large dense matrix will consume tons of memory.\nThe formula portion at issue is where a matrix is added to some scalars.\nA = V + x\nB = A + y\nWhere V is a square sparse matrix (its large, say 60,000 x 60,000).\nWhat I want is that x, y will only be added to non-zero values in V.\nWith a SciPy, not all sparse matrices support the same features, like scalar addition. dok_matrix (Dictionary of Keys) supports scalar addition, but it looks like (in practice) that it's allocating each matrix entry, effectively rendering my sparse dok_matrix as a dense matrix with more overhead. (not good)\nThe other matrix types (CSR, CSC, LIL) don't support scalar addition.\nI could try constructing a full matrix with the scalar value x, then adding that to V. I would have no problems with matrix types as they all seem to support matrix addition. However I would have to eat up a lot of memory to construct x as a matrix, and the result of the addition could end up being fully populated matrix as well.\nThere must be an alternative way to do this that doesn't require allocating 100% of a sparse matrix. I\u2019d like to solve the problem on coo matrix first.\nI'm will to accept that large amounts of memory are needed, but I thought I would seek some advice first. Thanks.\nA:\n<code>\nfrom scipy import sparse\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\ny = 99\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(V)\n</code>\n\n\ndef",
        "answer": "def add_scalar(x, y):\n    return x + y"
    },
    {
        "library_id": 88,
        "source_id": 88,
        "prompt": "Complete the Python function given the prompt below:Problem:\nBasically, I am just trying to do a simple matrix multiplication, specifically, extract each column of it and normalize it by dividing it with its length.\n    #csc sparse matrix\n    self.__WeightMatrix__ = self.__WeightMatrix__.tocsc()\n    #iterate through columns\n    for Col in xrange(self.__WeightMatrix__.shape[1]):\n       Column = self.__WeightMatrix__[:,Col].data\n       List = [x**2 for x in Column]\n       #get the column length\n       Len = math.sqrt(sum(List))\n       #here I assumed dot(number,Column) would do a basic scalar product\n       dot((1/Len),Column)\n       #now what? how do I update the original column of the matrix, everything that have been returned are copies, which drove me nuts and missed pointers so much\nI've searched through the scipy sparse matrix documentations and got no useful information. I was hoping for a function to return a pointer/reference to the matrix so that I can directly modify its value. Thanks\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\nsa = sparse.random(10, 10, density = 0.3, format = 'csc', random_state = 42)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(sa)\n</code>\n\n\ndef",
        "answer": "def MatrixMultiplication(self, other):\n    #csc sparse matrix\n    self.__WeightMatrix__ = self.__WeightMatrix__.tocsc()\n    #iterate through columns\n    for Col in xrange(self.__WeightMatrix__.shape[1]):\n       Column = self.__WeightMatrix__[:,Col].data\n       List = [x*y for x,y in zip(Column,other)]\n       #get the column length\n       Len = math.sqrt(sum(List))\n       #here I assumed dot(number,Column) would do a basic scalar product\n       dot((1/Len),Column)\n       #now what? how do I update the original column of the matrix, everything that have been returned are copies, which drove me nuts and missed pointers so much\nI've searched through the scipy sparse matrix documentations and got no useful information. I was hoping for a function to return a pointer/reference to the matrix so that I can directly modify its value. Thanks\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\nsa = sparse.random(10, 10, density = 0.3, format = 'csc', random_state = 42)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(sa)\n</code>"
    },
    {
        "library_id": 89,
        "source_id": 88,
        "prompt": "Complete the Python function given the prompt below:Problem:\nBasically, I am just trying to do a simple matrix multiplication, specifically, extract each column of it and normalize it by dividing it with its length.\n    #csr sparse matrix\n    self.__WeightMatrix__ = self.__WeightMatrix__.tocsr()\n    #iterate through columns\n    for Col in xrange(self.__WeightMatrix__.shape[1]):\n       Column = self.__WeightMatrix__[:,Col].data\n       List = [x**2 for x in Column]\n       #get the column length\n       Len = math.sqrt(sum(List))\n       #here I assumed dot(number,Column) would do a basic scalar product\n       dot((1/Len),Column)\n       #now what? how do I update the original column of the matrix, everything that have been returned are copies, which drove me nuts and missed pointers so much\nI've searched through the scipy sparse matrix documentations and got no useful information. I was hoping for a function to return a pointer/reference to the matrix so that I can directly modify its value. Thanks\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\nsa = sparse.random(10, 10, density = 0.3, format = 'csr', random_state = 42)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(sa)\n</code>\n\n\ndef",
        "answer": "def MatrixMultiplication(self, other):\n    #csr sparse matrix\n    self.__WeightMatrix__ = self.__WeightMatrix__.tocsr()\n    #iterate through columns\n    for Col in xrange(self.__WeightMatrix__.shape[1]):\n       Column = self.__WeightMatrix__[:,Col].data\n       List = [x*y for x,y in zip(Column,other)]\n       #get the column length\n       Len = math.sqrt(sum(List))\n       #here I assumed dot(number,Column) would do a basic scalar product\n       dot((1/Len),Column)\n       #now what? how do I update the original column of the matrix, everything that have been returned are copies, which drove me nuts and missed pointers so much\nI've searched through the scipy sparse matrix documentations and got no useful information. I was hoping for a function to return a pointer/reference to the matrix so that I can directly modify its value. Thanks\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\nsa = sparse.random(10, 10, density = 0.3, format = 'csr', random_state = 42)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(sa)\n</code>"
    },
    {
        "library_id": 90,
        "source_id": 90,
        "prompt": "Complete the Python function given the prompt below:Problem:\nSuppose I have a integer matrix which represents who has emailed whom and how many times. For social network analysis I'd like to make a simple undirected graph. So I need to convert the matrix to binary matrix.\nMy question: is there a fast, convenient way to reduce the decimal matrix to a binary matrix.\nSuch that:\n26, 3, 0\n3, 195, 1\n0, 1, 17\nBecomes:\n1, 1, 0\n1, 1, 1\n0, 1, 1\nA:\n<code>\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(a)\n</code>\n\n\ndef",
        "answer": "def matrix_to_binary(matrix):\n    \"\"\"\n    Convert a matrix to a binary matrix.\n    \"\"\"\n    return np.array([[int(i) for i in row] for row in matrix])"
    },
    {
        "library_id": 91,
        "source_id": 90,
        "prompt": "Complete the Python function given the prompt below:Problem:\nSuppose I have a integer matrix which represents who has emailed whom and how many times. I want to find people that have not emailed each other. For social network analysis I'd like to make a simple undirected graph. So I need to convert the matrix to binary matrix.\nMy question: is there a fast, convenient way to reduce the decimal matrix to a binary matrix.\nSuch that:\n26, 3, 0\n3, 195, 1\n0, 1, 17\nBecomes:\n0, 0, 1\n0, 0, 0\n1, 0, 0\nA:\n<code>\nimport scipy\nimport numpy as np\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(a)\n</code>\n\n\ndef",
        "answer": "def matrix_to_binary(matrix):\n    return np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0"
    },
    {
        "library_id": 92,
        "source_id": 92,
        "prompt": "Complete the Python function given the prompt below:Problem:\nAfter clustering a distance matrix with scipy.cluster.hierarchy.linkage, and assigning each sample to a cluster using scipy.cluster.hierarchy.cut_tree, I would like to extract one element out of each cluster, which is the closest to that cluster's centroid.\n\u2022    I would be the happiest if an off-the-shelf function existed for this, but in the lack thereof:\n\u2022    some suggestions were already proposed here for extracting the centroids themselves, but not the closest-to-centroid elements.\n\u2022    Note that this is not to be confused with the centroid linkage rule in scipy.cluster.hierarchy.linkage. I have already carried out the clustering itself, just want to access the closest-to-centroid elements.\nWhat I want is the index of the closest element in original data for each cluster, i.e., result[0] is the index of the closest element to cluster 0.\nA:\n<code>\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def distance(x, y):\n    return np.sqrt(np.sum((x - y) ** 2))"
    },
    {
        "library_id": 93,
        "source_id": 92,
        "prompt": "Complete the Python function given the prompt below:Problem:\nAfter clustering a distance matrix with scipy.cluster.hierarchy.linkage, and assigning each sample to a cluster using scipy.cluster.hierarchy.cut_tree, I would like to extract one element out of each cluster, which is the closest to that cluster's centroid.\n\u2022    I would be the happiest if an off-the-shelf function existed for this, but in the lack thereof:\n\u2022    some suggestions were already proposed here for extracting the centroids themselves, but not the closest-to-centroid elements.\n\u2022    Note that this is not to be confused with the centroid linkage rule in scipy.cluster.hierarchy.linkage. I have already carried out the clustering itself, just want to access the closest-to-centroid elements.\nWhat I want is the vector of the closest point to each cluster, i.e., result[0] is the vector of the closest element to cluster 0.\nA:\n<code>\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def distance(x, y):\n    return np.sqrt(np.sum((x - y) ** 2))"
    },
    {
        "library_id": 94,
        "source_id": 92,
        "prompt": "Complete the Python function given the prompt below:Problem:\nAfter clustering a distance matrix with scipy.cluster.hierarchy.linkage, and assigning each sample to a cluster using scipy.cluster.hierarchy.cut_tree, I would like to extract one element out of each cluster, which is the k-th closest to that cluster's centroid.\n\u2022    I would be the happiest if an off-the-shelf function existed for this, but in the lack thereof:\n\u2022    some suggestions were already proposed here for extracting the centroids themselves, but not the closest-to-centroid elements.\n\u2022    Note that this is not to be confused with the centroid linkage rule in scipy.cluster.hierarchy.linkage. I have already carried out the clustering itself, just want to access the closest-to-centroid elements.\nWhat I want is the index of the k-closest element in original data for each cluster, i.e., result[0] is the index of the k-th closest element to centroid of cluster 0.\nA:\n<code>\nimport numpy as np\nimport scipy.spatial\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def distance(x, y):\n    return np.sqrt(np.sum((x - y) ** 2))"
    },
    {
        "library_id": 95,
        "source_id": 95,
        "prompt": "Complete the Python function given the prompt below:Problem:\nScipy offers many useful tools for root finding, notably fsolve. Typically a program has the following form:\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nfsolve(eqn, x0=0.5, args = (a,b))\nand will find a root for eqn(x) = 0 given some arguments a and b.\nHowever, what if I have a problem where I want to solve for the a variable, giving the function arguments in x and b? Of course, I could recast the initial equation as\ndef eqn(a, x, b)\nbut this seems long winded and inefficient. Instead, is there a way I can simply set fsolve (or another root finding algorithm) to allow me to choose which variable I want to solve for?\nNote that the result should be an array of roots for many (x, b) pairs.\nA:\n<code>\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def eqn(x, a, b):\n    return x + 2*a - b**2\nresult = fsolve(eqn, x0=0.5, args=(a,b))\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nEND SOLUTION\n<"
    },
    {
        "library_id": 96,
        "source_id": 95,
        "prompt": "Complete the Python function given the prompt below:Problem:\nScipy offers many useful tools for root finding, notably fsolve. Typically a program has the following form:\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nfsolve(eqn, x0=0.5, args = (a,b))\nand will find a root for eqn(x) = 0 given some arguments a and b.\nHowever, what if I have a problem where I want to solve for the b variable, giving the function arguments in a and b? Of course, I could recast the initial equation as\ndef eqn(b, x, a)\nbut this seems long winded and inefficient. Instead, is there a way I can simply set fsolve (or another root finding algorithm) to allow me to choose which variable I want to solve for?\nNote that the result should be an array of roots for many (x, a) pairs. The function might have two roots for each setting, and I want to put the smaller one first, like this:\nresult = [[2, 5],\n          [-3, 4]] for two (x, a) pairs\nA:\n<code>\nimport numpy as np\nfrom scipy.optimize import fsolve\ndef eqn(x, a, b):\n    return x + 2*a - b**2\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def eqn(x, a, b):\n    return x + 2*a - b**2\nfsolve(eqn, x0=0.5, args = (a,b))\nprint(result)\n</code>"
    },
    {
        "library_id": 97,
        "source_id": 97,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have an array of experimental values and a probability density function that supposedly describes their distribution:\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nI estimated the parameters of my function using scipy.optimize.curve_fit and now I need to somehow test the goodness of fit. I found a scipy.stats.kstest function which suposedly does exactly what I need, but it requires a continuous distribution function. \nHow do I get the result (statistic, pvalue) of KStest? I have some sample_data from fitted function, and parameters of it.\nA:\n<code>\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 98,
        "source_id": 97,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have an array of experimental values and a probability density function that supposedly describes their distribution:\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nI estimated the parameters of my function using scipy.optimize.curve_fit and now I need to somehow test the goodness of fit. I found a scipy.stats.kstest function which suposedly does exactly what I need, but it requires a continuous distribution function. \nHow do I get the result of KStest? I have some sample_data from fitted function, and parameters of it.\nThen I want to see whether KStest result can reject the null hypothesis, based on p-value at 95% confidence level.\nHopefully, I want `result = True` for `reject`, `result = False` for `cannot reject`\nA:\n<code>\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>"
    },
    {
        "library_id": 99,
        "source_id": 99,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI want to capture an integral of a column of my dataframe with a time index. This works fine for a grouping that happens every time interval.\nfrom scipy import integrate\n>>> df\nTime                      A\n2017-12-18 19:54:40   -50187.0\n2017-12-18 19:54:45   -60890.5\n2017-12-18 19:54:50   -28258.5\n2017-12-18 19:54:55    -8151.0\n2017-12-18 19:55:00    -9108.5\n2017-12-18 19:55:05   -12047.0\n2017-12-18 19:55:10   -19418.0\n2017-12-18 19:55:15   -50686.0\n2017-12-18 19:55:20   -57159.0\n2017-12-18 19:55:25   -42847.0\n>>> integral_df = df.groupby(pd.Grouper(freq='25S')).apply(integrate.trapz)\nTime                       A\n2017-12-18 19:54:35   -118318.00\n2017-12-18 19:55:00   -115284.75\n2017-12-18 19:55:25         0.00\nFreq: 25S, Name: A, dtype: float64\nEDIT:\nThe scipy integral function automatically uses the time index to calculate it's result.\nThis is not true. You have to explicitly pass the conversion to np datetime in order for scipy.integrate.trapz to properly integrate using time. See my comment on this question.\nBut, i'd like to take a rolling integral instead. I've tried Using rolling functions found on SO, But the code was getting messy as I tried to workout my input to the integrate function, as these rolling functions don't return dataframes.\nHow can I take a rolling integral over time over a function of one of my dataframe columns?\nA:\n<code>\nimport pandas as pd\nimport io\nfrom scipy import integrate\nstring = '''\nTime                      A\n2017-12-18-19:54:40   -50187.0\n2017-12-18-19:54:45   -60890.5\n2017-12-18-19:54:50   -28258.5\n2017-12-18-19:54:55    -8151.0\n2017-12-18-19:55:00    -9108.5\n2017-12-18-19:55:05   -12047.0\n2017-12-18-19:55:10   -19418.0\n2017-12-18-19:55:15   -50686.0\n2017-12-18-19:55:20   -57159.0\n2017-12-18-19:55:25   -42847.0\n'''\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+')\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(integral_df)\n</code>\n\n\ndef",
        "answer": "def integral_df(df):\n    return df.groupby(pd.Grouper(freq='25S')).apply(integrate.trapz)"
    },
    {
        "library_id": 100,
        "source_id": 100,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have two data points on a 2-D image grid and the value of some quantity of interest at these two points is known.\nFor example:\nLet us consider the point being x=(2,2). Then considering a 4-grid neighborhood we have points x_1=(1,2), x_2=(2,3), x_3=(3,2), x_4=(2,1) as neighbours of x. Suppose the value of some quantity of interest at these points be y=5, y_1=7, y_2=8, y_3= 10, y_4 = 3. Through interpolation, I want to find y at a sub-pixel value, say at (2.7, 2.3). The above problem can be represented with numpy arrays as follows.\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\nHow to use numpy/scipy linear interpolation to do this? I want result from griddata in scipy.\nA:\n<code>\nimport scipy.interpolate\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\neval = [(2.7, 2.3)]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def griddata(x,y,eval):\n    import numpy as np\n    import scipy.interpolate\n    x = np.array(x)\n    y = np.array(y)\n    eval = np.array(eval)\n    result = scipy.interpolate.griddata(x,y,eval)\n    return result"
    },
    {
        "library_id": 101,
        "source_id": 101,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI just start learning Python. Here is a data frame:\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\nNow I think this data follows multinomial distribution. So, 12 numbers means the frequency of 12 categories (category 0, 1, 2...). For example, the occurance of category 0 is 0. So, I hope to find all the parameters of multinomial given this data. In the end, we have the best parameters of multinomial (or we can say the best probility for every number). For example,\ncategory:    0,      1,     2,     3,      4...\nweights:    0.001,  0.1,   0.2,   0.12,   0.2...\nSo, I do not need a test data to predict. Could anyone give me some help?\nI know that Maximum Likelihood Estimation is one of the most important procedure to get point estimation for parameters of a distribution. So how can I apply it to this question?\nA:\n<code>\nimport scipy.optimize as sciopt\nimport numpy as np\nimport pandas as pd\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(weights)\n</code>\n\n\ndef",
        "answer": "def func(x,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x):\n    return a*x+b*x**2+c*x**3+d*x**4+e*x**5+f*x**6+g*x**7+h*x**8+i*x**9+j*x**10+k*x**11+l*x**12+m*x**13+n*x**14+o*x**15+p*x**16+q*x**17+r*x**18+s*x**19+t*x**20+u*x**21+v*x**22+w*x**23+x"
    },
    {
        "library_id": 102,
        "source_id": 102,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI am trying to optimise a function using the fminbound function of the scipy.optimize module. I want to set parameter bounds to keep the answer physically sensible (e.g. > 0).\nimport scipy.optimize as sciopt\nimport numpy as np\nThe arrays:\nx = np.array([[ 1247.04,  1274.9,  1277.81,  1259.51,  1246.06,  1230.2,\n     1207.37,  1192. ,  1180.84,  1182.76,  1194.76,  1222.65],\n   [  589. ,   581.29,   576.1,   570.28,   566.45,   575.99,\n      601.1,   620.6,   637.04,   631.68,   611.79,   599.19]])\ny = np.array([ 1872.81,  1875.41,  1871.43,  1865.94,  1854.8,  1839.2,\n    1827.82,  1831.73,  1846.68,  1856.56,  1861.02,  1867.15])\nI managed to optimise the linear function within the parameter bounds when I use only one parameter:\nfp   = lambda p, x: x[0]+p*x[1]\ne    = lambda p, x, y: ((fp(p,x)-y)**2).sum()\npmin = 0.5 # mimimum bound\npmax = 1.5 # maximum bound\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nThis results in popt = 1.05501927245\nHowever, when trying to optimise with multiple parameters, I get the following error message:\nfp   = lambda p, x: p[0]*x[0]+p[1]*x[1]\ne    = lambda p, x, y: ((fp(p,x)-y)**2).sum()\npmin = np.array([0.5,0.5]) # mimimum bounds\npmax = np.array([1.5,1.5]) # maximum bounds\npopt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/python2.7/dist-packages/scipy/optimize/optimize.py\", line 949, in fminbound\n    if x1 > x2:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nI have tried to vectorize e (np.vectorize) but the error message remains the same. I understand that fminbound expects a float or array scalar as bounds. Is there another function that would work for this problem? The result should be solutions for p[0] and p[1] that minimize the objective function.\nA:\n<code>\nimport numpy as np\nimport scipy.optimize as sciopt\nx = np.array([[ 1247.04,  1274.9,  1277.81,  1259.51,  1246.06,  1230.2,\n     1207.37,  1192. ,  1180.84,  1182.76,  1194.76,  1222.65],\n   [  589. ,   581.29,   576.1,   570.28,   566.45,   575.99,\n      601.1,   620.6,   637.04,   631.68,   611.79,   599.19]])\ny = np.array([ 1872.81,  1875.41,  1871.43,  1865.94,  1854.8,  1839.2,\n    1827.82,  1831.73,  1846.68,  1856.56,  1861.02,  1867.15])\nfp   = lambda p, x: p[0]*x[0]+p[1]*x[1]\ne    = lambda p, x, y: ((fp(p,x)-y)**2).sum()\npmin = np.array([0.5,0.7]) # mimimum bounds\npmax = np.array([1.5,1.8]) # maximum bounds\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def f(x):\n    return np.array([x[0]+x[1]*x[2], x[1]+x[2]*x[3], x[2]+x[3]*x[4], x[3]+x[4]*x[5], x[4]+x[5]*x[6], x[5]+x[6]*x[7], x[6]+x[7]*x[8], x[7]+x[8]*x[9], x[8]+x[9]*x[10], x[9]+x[10]*x[11], x[10]+x[11]*x[12], x[11]+x[12]*x[13], x[12]+x[13]*x[14], x[13]+x[14]*x[15], x[14]+x[15]*x[16], x[15]+x[16]*x[17], x[16]+x[17]*x[18], x[17]+x[18]*x[19], x[18]+x[19]*x[20], x[19]+x[20]*x[21], x[20]+x[21]*x[22], x[21]+x[22]*x[23], x[22]+x[23]*x[24], x[23]+x[24]*x[25], x[24]+x[25]*x[26], x[25]+x[26]*x[27], x[26]+x[27]*x[28], x[27]+x[28]*x[29], x[28]+x[29]*x[30], x[29]+x[30]*x[31], x[30]+x[31]*x[32], x[31]+x[32]*x[33], x[32]+x[33]*x[34], x[33]+x[34]*x[35], x[34]+x[35]*x[36], x[35]+x[36]*x[37], x[36]+x[37]*x[38], x[37]+x[38]"
    },
    {
        "library_id": 103,
        "source_id": 103,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow to find relative extrema of a given array? An element is a relative extrema if it is less or equal to the neighbouring n (e.g. n = 2) elements forwards and backwards. The result should be an array of indices of those elements in original order.\nA:\n<code>\nimport numpy as np\nfrom scipy import signal\narr = np.array([-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,])\nn = 2\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def find_extrema(arr):\n    result = []\n    for i in range(len(arr)):\n        for j in range(i + 1, len(arr)):\n            if arr[i] <= arr[j]:\n                result.append(i)\n                result.append(j)\n    return result"
    },
    {
        "library_id": 104,
        "source_id": 103,
        "prompt": "Complete the Python function given the prompt below:Problem:\nHow to find relative extrema of a 2D array? An element is a relative extrema if it is less or equal to the neighbouring n (e.g. n = 2) elements forwards and backwards in the row. \nThe result should be a list of indices of those elements, [0, 1] stands for arr[0][1]. It should be arranged like\n[[0, 1], [0, 5], [1, 1], [1, 4], [2, 3], [2, 5],...]\nA:\n<code>\nimport numpy as np\nfrom scipy import signal\narr = np.array([[-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,], [3, 0, 0, 1, 2, 4]])\nn = 2\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\n\n\ndef",
        "answer": "def find_relative_extrema(arr, n):\n    result = []\n    for i in range(n):\n        for j in range(n):\n            if arr[i][j] >= arr[i][j + 1] and arr[i][j] >= arr[i + 1][j]:\n                result.append([i, j])\n    return result"
    },
    {
        "library_id": 105,
        "source_id": 105,
        "prompt": "Complete the Python function given the prompt below:Problem:\nI have a data-set which contains many numerical and categorical values, and I want to only test for outlying values on the numerical columns and remove rows based on those columns.\nI am trying it like this:\ndf = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\nWhere it will remove all outlying values in all columns, however of course because I have categorical columns I am met with the following error:\nTypeError: unsupported operand type(s) for +: 'float' and'str'\nI know the solution above works because if I limit my df to only contain numeric columns it all works fine but I don't want to lose the rest of the information in my dataframe in the process of evaluating outliers from numeric columns.\nA:\n<code>\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\ndf = pd.DataFrame({'NUM1': np.random.randn(50)*100,\n                   'NUM2': np.random.uniform(0,1,50),                   \n                   'NUM3': np.random.randint(100, size=50),                                             \n                   'CAT1': [\"\".join(np.random.choice(LETTERS,1)) for _ in range(50)],\n                   'CAT2': [\"\".join(np.random.choice(['pandas', 'r', 'julia','sas','stata','spss'],1)) for _ in range(50)],              \n                   'CAT3': [\"\".join(np.random.choice(['postgres','mysql','sqlite', 'oracle','sql server', 'db2'],1)) for _ in range(50)]\n                  })\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df)\n</code>\n\n\ndef",
        "answer": "def test_outliers(df):\n    # I am trying to remove outliers from the numerical columns\n    # I am trying to remove outliers from the categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I am trying to remove outliers from the numerical and categorical columns\n    # I"
    }
]